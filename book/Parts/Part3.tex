\part{Part 3: Numerics}

\chapterimage{chapter_head_2.pdf} % Chapter heading image

\chapter{Taylor-Series}
To begin our exploration of CFD we will state with the Finite Difference Method (FDM). This approach uses conservation laws in {\it divergence form}. We start by recalling a few of the simplified conservation laws we derived in the previous section, specifically the linear advection, Burgers, and linear diffusion equations
\begin{eqBox}
\begin{equation}
	\frac{\partial u}{\partial t} +  \alpha \frac{\partial u}{\partial x} = 0,
\end{equation}
\begin{equation}
	\frac{\partial u}{\partial t} +  \frac{1}{2} \frac{\partial u^2}{\partial x} = 0,
\end{equation}
\begin{equation}
	\frac{\partial u}{\partial t} - \alpha \frac{\partial^2 u}{\partial x^2} = 0.
\end{equation}
\end{eqBox}
We notice that the form of all of these equations is very similar. All three involve a time derivative combined with a coefficient multiplied by a spatial derivative. The Finite Difference Method uses well-known concepts from applied mathematics, specifically Taylor-Series. Hence, before deriving the Finite Difference Method we will start by reviewing some concepts from Taylor-Series.


Arising from Taylors theorem, Taylor series says that the value of a sufficiently smooth function at some point $x + \Delta x$ can be predicted by using the value of the solution at the point $x$ along with knowledge of all derivatives at the point $x$.
\begin{theorem}[Taylor's Theorem]
Let $k \geq 1$ and letting $f(x)$ be smooth and differentiable $k$ times then
\begin{align}
f(x + \Delta x) = f(x) + \frac{\partial f}{\partial x}\frac{\Delta x^1}{1!} + \frac{\partial^2 f}{\partial x^2}\frac{\Delta x^2}{2!} + \hdots + \frac{\partial^k f}{\partial x^k}\frac{\Delta x^k}{k!},
\end{align}
which can be written compactly for an expansion truncated to n terms as
\begin{align}
f(x + \Delta x) = \sum_{i=0}^{k} \frac{\partial^i f}{\partial x^i}\frac{\Delta x^i}{i!}.
\end{align}
\end{theorem}
While there are some limitations, particularly around the smoothness of the function and its derivatives, it is worth taking a moment to consider just how powerful Taylor series can be. It allows us to represent the entirety of a smooth function using the value of the solution and its derivatives at only one point in the domain. In addition, similar to Fourier series, we often find that a truncated expansion, which omits high-order terms in the summation, is sufficient for many applications.

We will demonstrate the utility of Taylor series via example. Consider a simple sine wave 
\begin{equation}
	f(x) = \sin(x),
\end{equation}
which is periodic on the interval $[- \pi, \pi]$. We first note that sine functions are sufficiently smooth for Taylor series to be applied up to an infinite number of derivatives. Now we will explore the behaviour of the Taylor series as we add more terms. When we use a finite number of terms in the expansion we are {\it truncating} all of the higher-order terms. We will form our expansion about the point $x = 0$. If we include just the first term and truncate all others we obtain
\begin{equation}
	f_0(\Delta x) = 0,
\end{equation}
which is not a particularly accurate approximation of a sine wave except very close to the origin. However, as we start to add more terms we obtain the following expressions,
\begin{equation}
	f_1(\Delta x) = \Delta x,
\end{equation}
\begin{equation}
	f_3(\Delta x) = \Delta x - \frac{\Delta x^3}{6},
\end{equation}
\begin{equation}
	f_5(\Delta x) = \Delta x - \frac{\Delta x^3}{6} + \frac{\Delta x^5}{120},
\end{equation}
\begin{equation}
	f_7(\Delta x) = \Delta x - \frac{\Delta x^3}{6} + \frac{\Delta x^5}{120} - \frac{\Delta x^7}{5040},
\end{equation}
\begin{equation}
	f_{9}(\Delta x) = \Delta x - \frac{\Delta x^3}{6} + \frac{\Delta x^5}{120} - \frac{\Delta x^7}{5040} + \frac{\Delta x^9}{362880},
\end{equation}
where the subscript denotes the power of the highest degree expansion term included in the approximation. Each of these is a polynomial approximation of a sine wave about the point $x=0$. We can make a few observations about the behaviour of Taylor series in general. First, when we are close to the expansion point $x=0$, even approximations with a small number of terms are close to the true function. For example, the $f_1$ expansion is the well-known small angle approximation of a sine wave. Then, as we add more terms the accuracy of the expansion improves rapidly. For example, by ten terms the approximation is nearly indistinguishable from the true function on the entire interval. Hence, we note that Taylor series becomes more accurate as we get closer to the expansion point and/or as we increase the number of terms.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\linewidth]{Pictures/ch9_taylor_series_sin}
	\caption{Taylor series expansion of $sin(x)$ about $x=0$}
	\label{fig:taylor_series}
\end{figure}

To understand the behaviour of these truncated expansions, we note that their error arises from truncating the higher-order terms of the expansion. Furthermore, when $\Delta x$ is relatively small the error is dominated by the first truncated term, since the higher-order terms rapidly approximate zero as $\Delta x$ decreases. Hence, if we consider an infinite expansion
\begin{equation}
	f(\Delta x) = \Delta x - \frac{\Delta x^3}{6} + \frac{\Delta x^5}{120} - \frac{\Delta x^7}{5040} + \frac{\Delta x^9}{362880} - \frac{\Delta x^{11}}{39916800} + \hdots,
\end{equation}
we note that the leading term omitted from the $f_1(\Delta x)$ approximation behaves like $\Delta x^3$, the leading error term of the $f_1(\Delta x)$ approximation behaves like $\Delta x^5$, and so on. Hence, as we get closer to the expansion point $x=0$ we expect that the error shrinks, and that the rate at which the error shrinks will be proportional to the exponent of the leading truncated term of the expansion. We can test this by creating a convergence table.
	
\chapter{Finite Difference Methods}
\section{The First Derivative}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\linewidth]{Pictures/ch10_fd_scheme}
	\caption{}
	\label{fig:fd_scheme}
\end{figure}
	
To introduce the finite difference method we start with a general function $u(x)$. In general this function can be in any number of dimensions, but for simplicity we will first consider the one-dimensional case. We note that a function can generally be {\it approximated} by its values at a {\it discrete} set of points. An example of this is shown in Figure \ref{fig:fd_scheme} where $\Delta x$ is the grid spacing between the points. We now imagine that we are at some point $i$ whose solution is $u_i = u(x_i)$ where $x_i$ is the physical location of the point. Using Taylor series we can compute the value of the solution one grid point to the left from the expansion
\begin{equation}
	u_{i-1} = u_i - \Delta x \Eval{\frac{\partial u}{\partial x}}{x_i}{} + \frac{\Delta x^2}{2} \Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} - \frac{\Delta x^3}{6} \Eval{\frac{\partial^3 u}{\partial x^3}}{x_i}{} + \mathcal{O}(\Delta x^4),
	\label{eqn:fdmup}
\end{equation}
where $\mathcal{O}(\Delta x^4)$ denotes the order of the next highest term in the expansion. We can also use another Taylor series to predict the value of the solution one point to the right
\begin{equation}
	u_{i+1} = u_i + \Delta x \Eval{\frac{\partial u}{\partial x}}{x_i}{} + \frac{\Delta x^2}{2} \Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} + \frac{\Delta x^3}{6} \Eval{\frac{\partial^3 u}{\partial x^3}}{x_i}{} + \mathcal{O}(\Delta x^4).
	\label{eqn:fdmdwn}
\end{equation}

So far, we have just applied Taylor series to try to determine the solution at points around our initial point $u_i$. However, if we look at Equation \ref{eqn:fdmup} we can use it in a different way. Rearranging we get
\begin{equation}
	\Eval{\frac{\partial u}{\partial x}}{x_i}{} = \frac{u_i - u_{i-1}}{\Delta x} + \frac{\Delta x}{2} \Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} - \frac{\Delta x^2}{6} \Eval{\frac{\partial^3 u}{\partial x^3}}{x_i}{} + \mathcal{O}(\Delta x^3).
	\label{eqn:fdmup1}
\end{equation}
We note that as the grid spacing gets smaller the terms with leading $\Delta x$, $\Delta x^2$, and so on will tend to zero and the approximation
\begin{eqBox}
\begin{equation}
	\Eval{\frac{\partial u}{\partial x}}{x_i}{} = \frac{u_i - u_{i-1}}{\Delta x} + \mathcal{O}(\Delta x),
\end{equation}
\end{eqBox}
will converge to the true value of the derivative at the point $x_i$ as $\Delta x$ tends to zero. We can also do the same trick with Equation \ref{eqn:fdmdwn}. Rearranging we get
\begin{equation}
	\Eval{\frac{\partial u}{\partial x}}{x_i}{} = \frac{u_{i+1} - u_{i}}{\Delta x} - \frac{\Delta x}{2} \Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} - \frac{\Delta x^2}{6} \Eval{\frac{\partial^3 u}{\partial x^3}}{x_i}{} + \mathcal{O}(\Delta x^3),
	\label{eqn:fdmdwn1}
\end{equation}
and again if the grid spacing is small then the higher-order terms tend to zero and we are left with
\begin{eqBox}
\begin{equation}
	\Eval{\frac{\partial u}{\partial x}}{x_i}{} = \frac{u_{i+1} - u_{i}}{\Delta x} + \mathcal{O}(\Delta x),
\end{equation}
\end{eqBox}
which also allows us to approximate the derivative at $x_i$. Hence, if we know that value of the solution at $x_i$ and either $x_{i-1}$ or $x_{i+1}$ we can approximate the derivative at $x_i$. We note that for both of these approximations the error in the derivative approximation will decrease proportional to the grid spacing, since the leading term in the truncation error is of $\mathcal{O}(\Delta x)$. We refer to a scheme of this form as a {\it first-order} finite difference method.

While the above examples are useful they converge rather slowly to the exact value of the derivative at the point $x_i$. However, if we take an average of Equation \ref{eqn:fdmup1} and Equation \ref{eqn:fdmdwn1} we obtain
\begin{equation}
	\begin{aligned}
	\Eval{\frac{\partial u}{\partial x}}{x_i}{} = &\frac{1}{2}\left(\frac{u_i - u_{i-1}}{\Delta x} + \frac{\Delta x}{2} \Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} - \frac{\Delta x^2}{6} \Eval{\frac{\partial^3 u}{\partial x^3}}{x_i}{} \right) + \\
	& \frac{1}{2}\left(\frac{u_{i+1} - u_{i}}{\Delta x} - \frac{\Delta x}{2} \Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} - \frac{\Delta x^2}{6} \Eval{\frac{\partial^3 u}{\partial x^3}}{x_i}{} \right) + \mathcal{O}(\Delta x^3)
	\end{aligned}
\end{equation}
which simplifies to
\begin{equation}
	\Eval{\frac{\partial u}{\partial x}}{x_i}{} = \frac{u_{i+1} - u_{i-1}}{2 \Delta x} - \frac{\Delta x^2}{6} \Eval{\frac{\partial^3 u}{\partial x^3}}{x_i}{} + \mathcal{O}(\Delta x^3),
\end{equation}
noting that the leading truncation error terms conveniently cancelled each other out. Hence, if we assume that $\Delta x$ is relatively small we obtain
\begin{eqBox}
\begin{equation}
	\Eval{\frac{\partial u}{\partial x}}{x_i}{} = \frac{u_{i+1} - u_{i-1}}{2 \Delta x} + \mathcal{O}(\Delta x^2).
\end{equation}
\end{eqBox}
Using this equation we can approximate the value of the derivative of the function at $x_i$ using the values of the solution at two neighbouring points $x_{i-1}$ and $x_{i+1}$. However, unlike the previous finite difference methods, the truncation error of this approximation is $\mathcal{O}(\Delta x^2)$, meaning that as the grid spacing is reduced the error will converge like the grid spacing squared. We refer to a scheme of this form as a {\it second-order} finite difference method.

\section{A General Approach}
In the previous section we derived three different finite difference methods for computing the derivative at a point $x_i$ using the values of the solution at that point and neighbouring points. We have shown that we can create at least two first-order schemes and one second-order scheme. In the current section we will demonstrate how to generalize this approach to allow us to obtain schemes of any order and for higher-order derivatives.

To start with, we will derive a second-order finite difference method to approximate the first derivative using $u_i$, $u_{i-1}$, and $u_{i-2}$. This procedure can be broken down into four basic steps.

\subsection{Step 1: Generate the Taylor Series}
The first step is to expand a Taylor series about each of the points that will be used to approximate the derivative. The Taylor series for $u_{i-1}$ was given before and is
\begin{equation}
	u_{i-1} = u_i - \Delta x \Eval{\frac{\partial u}{\partial x}}{x_i}{} + \frac{\Delta x^2}{2} \Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} - \frac{\Delta x^3}{6} \Eval{\frac{\partial^3 u}{\partial x^3}}{x_i}{} + \mathcal{O}(\Delta x^4).
	\label{fdm:general1}
\end{equation}
Similarly, since the distance between $u_i$ and $u_{i-2}$ is $2 \Delta x$ we get the following Taylor series for that point
\begin{equation}
	u_{i-2} = u_i - 2 \Delta x \Eval{\frac{\partial u}{\partial x}}{x_i}{} + \frac{(2\Delta x)^2}{2} \Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} - \frac{(2\Delta x)^3}{6} \Eval{\frac{\partial^3 u}{\partial x^3}}{x_i}{} + \mathcal{O}(\Delta x^4).
	\label{fdm:general2}
\end{equation}

\subsection{Step 2: Rearrange the Taylor Series}
The second step is to rearrange each of the Taylor series generated in Step 1 to obtain the derivative of interest on the LHS with everything else on the RHS. Since we are interested in finding an approximation for the first derivative we rearrange Equation \ref{fdm:general1}
\begin{equation}
	\Eval{\frac{\partial u}{\partial x}}{x_i}{} = \frac{u_i - u_{i-1}}{\Delta x} + \frac{\Delta x}{2}\Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} - \frac{\Delta x^2}{6}\Eval{\frac{\partial^3 u}{\partial x^3}}{x_i}{} + \mathcal{O}(\Delta x^3),
	\label{eqn:aHK8G4M7}
\end{equation}
and rearrange Equation \ref{fdm:general1}
\begin{equation}
	\Eval{\frac{\partial u}{\partial x}}{x_i}{} = \frac{u_i - u_{i-2}}{2\Delta x} + \frac{2\Delta x}{2}\Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} - \frac{(2\Delta x)^2}{6}\Eval{\frac{\partial^3 u}{\partial x^3}}{x_i}{} + \mathcal{O}(\Delta x^3),
	\label{eqn:6sbrhgAM}
\end{equation}
which gives two expressions for the first derivative.

\subsection{Step 3: Determine a Suitable Combination}
If we look at the previous two expressions for the first derivative we will notice that they are both first-order accurate, since the leading term that will get truncated is $\mathcal{O}(\Delta x)$. However, we were tasked with finding a second-order accurate scheme. In order to achieve this we will try to combine Equation \ref{eqn:aHK8G4M7} and Equation \ref{eqn:6sbrhgAM} in such a way the this first-order error term cancels out.

We want to combine them is such a way that
\begin{equation}
	\Eval{\frac{\partial u}{\partial x}}{x_i}{} = a(\ref{eqn:aHK8G4M7}) + b(\ref{eqn:6sbrhgAM})
	\label{eqn:2KGWrZa5}
\end{equation}
gives a second-order approximating for the first derivative, and we need to find the coefficients $a$ and $b$ to achieve this. If we look at the left hand side, we want to keep the first derivative there. This can be achieve if we ensure that
\begin{equation}
	a + b = 1.
	\label{eqn:3Wxy2aCD}
\end{equation}
The second thing we need to do is cancel out the $\mathcal{O}(\Delta x)$ term to ensure that the leading truncation error term is $\mathcal{O}(\Delta x^2)$. Looking at the $\mathcal{O}(\Delta x)$ terms, in order for them to cancel out we require
\begin{equation}
	\frac{a}{2} + b = 0,
	\label{eqn:rWD4Hs8v}
\end{equation}
to cancel them out. Equations \ref{eqn:3Wxy2aCD} and \ref{eqn:rWD4Hs8v} are recognizable as a linear system of equations with two equations and two unknowns. This can be readily solved using substitution yielding $a=2$ and $b=-1$.

\subsection{Step 3: Combine the Schemes}
Now that we have determine the constants in Equation \ref{eqn:2KGWrZa5}, the last step is to just go ahead and add things together. Doing this will yield that following finite difference approximation
\begin{eqBox}
\begin{equation}
	\Eval{\frac{\partial u}{\partial x}}{x_i}{} = \frac{3u_i - 4u_{i-1} + u_{i-2}}{2 \Delta x} + \mathcal{O}(\Delta x^2),
	\label{eqn:aHK8G4M7}
\end{equation}
\end{eqBox}
which is a second-order approximation of the first derivative use three points. Hence, we have accomplished our task.

\section{The Second Derivative}
In the previous sections we derived four difference schemes for the first derivative, two that were first-order accurate, and two that were second-order accurate. However, if we go back and look at the Navier-Stokes equations, we will note that we also need to approximate second-derivatives for the diffusive terms. We will derive and example scheme here, which also demonstrates another application of the four basic steps of creating a finite difference scheme. Our objective will be to derive a second-order finite difference method to approximate the second derivative using $u_i$, $u_{i-1}$, and $u_{i+1}$.

We now recall that the first step was to simply expand a Taylor series about all of the points that are not $u_i$. We have already derived these expansions for $u_{i-1}$ and $u_{i+1}$, which are
\begin{equation}
	u_{i-1} = u_i - \Delta x \Eval{\frac{\partial u}{\partial x}}{x_i}{} + \frac{\Delta x^2}{2} \Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} - \frac{\Delta x^3}{6} \Eval{\frac{\partial^3 u}{\partial x^3}}{x_i}{} + \mathcal{O}(\Delta x^4),
	\label{eqn:Bm6Tq8Ah}
\end{equation}
and
\begin{equation}
	u_{i+1} = u_i + \Delta x \Eval{\frac{\partial u}{\partial x}}{x_i}{} + \frac{\Delta x^2}{2} \Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} + \frac{\Delta x^3}{6} \Eval{\frac{\partial^3 u}{\partial x^3}}{x_i}{} + \mathcal{O}(\Delta x^4).
	\label{eqn:93Nxbhtu}
\end{equation}
So that is step one done.

In step two we needed to rearrange these equations such that the derivative of interest is on the LHS. This yields two possible expressions for the second derivative by rearranging Equation \ref{eqn:Bm6Tq8Ah}
\begin{equation}
	\Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} = \frac{2(u_{i-1} - u_i)}{\Delta x^2} + \frac{2}{\Delta x} \Eval{\frac{\partial u}{\partial x}}{x_i}{} + \frac{\Delta x}{3} \Eval{\frac{\partial^3 u}{\partial x^3}}{x_i}{} + \mathcal{O}(\Delta x^2),
\end{equation}
and Equation \ref{eqn:93Nxbhtu}
\begin{equation}
	\Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} = \frac{2(u_{i+1} - u_i)}{\Delta x^2} - \frac{2}{\Delta x} \Eval{\frac{\partial u}{\partial x}}{x_i}{} - \frac{\Delta x}{3} \Eval{\frac{\partial^3 u}{\partial x^3}}{x_i}{} + \mathcal{O}(\Delta x^2),
\end{equation}
noting that the $\mathcal{O}(\Delta x^4)$ terms are now $\mathcal{O}(\Delta x^2)$ since we had to divide the RHS by $\Delta x^2$. That marks the end of step two.

In the third step we have to determine a linear combination of these two equations that will give us an expression for the second derivative with a truncation error of $\mathcal{O}(\Delta x^2)$. Since we want to keep the second derivative on the LHS we require
\begin{equation}
	a + b = 1.
\end{equation}
And, in order for the truncation error to be of $\mathcal{O}(\Delta x^2)$ we need to cancel the second and third terms on the RHS of the equations, which are $\mathcal{O}(\Delta x^{-1})$ and $\mathcal{O}(\Delta x)$, respectively. Interestingly, cancelling out both of these terms requires
\begin{equation}
	a - b = 0.
\end{equation}
Once again, we have a linear system with two equations and two unknowns. Solving this system yields $a=b=1/2$, which finishes part three.

Finally, in part four we simply combine the two equations multiplied by their respective $a$ and $b$ coefficients. This yields our second-order accurate expression for the second-derivative as
\begin{eqBox}
\begin{equation}
	\Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} = \frac{u_{i-1} - 2u_i + u_{i+1}}{\Delta x^2} + \mathcal{O}(\Delta x^2).
\end{equation}
\end{eqBox}

\section{Example Applications}
Now that we have created a few introductory finite difference methods, it is time to put them to work on our three simplified systems.
\subsection{Linear Advection}
We recall that the one-dimensional linear advection equation in differential form was
\begin{equation}
	\frac{\partial u}{\partial t} +  \alpha \frac{\partial u}{\partial x} = 0.
\end{equation}
In the previous sections we derived a few options for this, but to start we will use the simple first-order upwind scheme to approximate the spatial derivative at each grid point, such that
\begin{equation}
	\Eval{\frac{\partial u}{\partial x}}{x_i}{} = \frac{u_i^t - u_{i-1}^t}{\Delta x} + \mathcal{O}(\Delta x),
\end{equation}
where the superscript $t$ denotes that we are using the solution at the current time. We will also use a first-order finite difference approach in time to approximate the temporal derivative as well. Hence,
\begin{equation}
	\Eval{\frac{\partial u}{\partial t}}{x_i}{} = \frac{u_i^{t+1} - u_{i}^t}{\Delta t} + \mathcal{O}(\Delta t),
\end{equation}
where $\Delta t$ is the time step size and the superscript $t+1$ is the solution at the next time step.

Now we simply insert these approximations into the linear advection equation, yielding
\begin{equation}
	\frac{u_i^{t+1} - u_{i}^t}{\Delta t} +  \alpha \frac{u_i^t - u_{i-1}^t}{\Delta x} + \mathcal{O}(\Delta x, \Delta t) = 0.
\end{equation}
By inspecting this equation, and assuming that we know the solution at each grid point at the current instant in time, we can rearrange this to make a prediction for the solution at the next time step
\begin{eqBox}
\begin{equation}
	u_i^{t+1} = u_{i}^t -\frac{\alpha \Delta t}{\Delta x} \left( u_i^t - u_{i-1}^t \right) + \mathcal{O}(\Delta x, \Delta t).
	\label{eqn:7hH8UBeG}
\end{equation}
\end{eqBox}

Looking at Equation \ref{eqn:7hH8UBeG} we note that, provided we know the current solution at all grid points at time $t$, we can predict the solution at time $t+\Delta t$ with an error term of $\mathcal{O}(\Delta x, \Delta t)$. Then we will have an approximation of the solution at all grid points at time $t+\Delta t$, and we can use this to find an approximation of the solution at time $t+2\Delta t$ by applying the same equation, and so on. Hence, we are able to advance the linear advection equation to any final time we desire, noting that each time step we take will introduce some numerical error.

\subsection{Burgers Equation}
We recall that the one-dimensional Burgers equation was
\begin{equation}
	\frac{\partial u}{\partial t} +  \frac{1}{2} \frac{\partial u^2}{\partial x} = 0.
\end{equation}
We can now use the exact same approach for Burgers as we used for the linear advection equation. First, we will use a first-order upwind approach to approximate the spatial derivative of $u^2$ at each grid point $x_i$, such that
\begin{equation}
	\Eval{\frac{\partial u^2}{\partial x}}{x_i}{} = \frac{(u_i^t)^2 - (u_{i-1}^t)^2}{\Delta x} + \mathcal{O}(\Delta x),
\end{equation}
where again the superscript $t$ denotes that we are using the solution at the current time. We will also use a first-order finite difference approach in time to approximate the temporal derivative as well. Hence,
\begin{equation}
	\Eval{\frac{\partial u}{\partial t}}{x_i}{} = \frac{u_i^{t+1} - u_{i}^t}{\Delta t} + \mathcal{O}(\Delta t).
\end{equation}

Now we simply insert our finite difference approximations of these two derivatives into the Burgers equation
\begin{equation}
	\frac{u_i^{t+1} - u_{i}^t}{\Delta t} +  \frac{1}{2}\frac{(u_i^t)^2 - (u_{i-1}^t)^2}{\Delta x} + \mathcal{O}(\Delta x, \Delta t) = 0,
\end{equation}
and, just like for linear advection, we can now find an expression for the solution at any grid point at the next time step as
\begin{eqBox}
\begin{equation}
	u_i^{t+1} = u_{i}^t - \frac{\Delta t}{\Delta x} \frac{1}{2}\left( \left(u_i^t \right)^2 - \left( u_{i-1}^t \right)^2 \right) + \mathcal{O}(\Delta x, \Delta t).
\end{equation}
\end{eqBox}
Looking at this expression we note it is remarkably similar to the expression for linear advection, which is not particularly surprising as the two initial partial differential equations are also quite similar. Again, if we know the solution at each grid point at some time $t$, we can approximate the solution at some time $t+\Delta t$ using this expression. Then, to approximate the solution at the time $t+2\Delta t$ we simply re-apply the expression. This allows us to approximate the behaviour of Burgers equation up to any desired final time, and the error of the approximation is expected to be first-order accurate in both space and time, with errors on the order of $\mathcal{O}(\Delta x, \Delta t)$.

\subsection{Linear Diffusion}
We recall that our third and final simplified system of equations was the linear diffusion equation
\begin{equation}
\frac{\partial u}{\partial t} - \beta \frac{\partial^2 u}{\partial x^2} = 0,
\end{equation}
which described how some conserved quantity, such as heat, diffuses into the surrounding fluid. To approximate this using the finite difference method we follow the exact same steps as for linear advection and Burgers equation. First, we will use our second-order accurate finite difference approximation for the second derivative as
\begin{equation}
	\Eval{\frac{\partial^2 u}{\partial x^2}}{x_i}{} = \frac{u_{i-1} - 2u_i + u_{i+1}}{\Delta x^2} + \mathcal{O}(\Delta x^2).
\end{equation}
We will also use a first-order finite difference approach in time to approximate the temporal derivative as well. Hence,
\begin{equation}
	\Eval{\frac{\partial u}{\partial t}}{x_i}{} = \frac{u_i^{t+1} - u_{i}^t}{\Delta t} + \mathcal{O}(\Delta t).
\end{equation}

Now we simply replace the exact derivatives in the linear diffusion equation with our finite difference approximations, such that
\begin{equation}
\frac{u_i^{t+1} - u_{i}^t}{\Delta t} - \beta \frac{u_{i-1} - 2u_i + u_{i+1}}{\Delta x^2} + \mathcal{O}(\Delta x^2,\Delta t) = 0.
\end{equation}
Again, by simply rearranging we can arrive at an expression for the solution at any grid point $i$ at the next time step as
\begin{eqBox}
\begin{equation}
u_i^{t+1}  = u_{i}^t + \frac{\beta \Delta t}{\Delta x^2} \left(u_{i-1} - 2u_i + u_{i+1}\right) + \mathcal{O}(\Delta x^2,\Delta t).
\end{equation}
\end{eqBox}
Again, this is very similar to the approximations for linear advection and Burgers equations. However, one important thing to note is that the error in space is of $\mathcal{O}(\Delta x^2)$ instead of $\mathcal{O}(\Delta x)$, meaning that as we refine the grid the solution will converge more quickly to the exact solution of the linear diffusion equation.

\chapter{Finite Volume Methods}
\begin{figure}[htbp]
 \centering
 \includegraphics[width=0.6\linewidth]{Pictures/ch11_fv_scheme}
 \caption{One-dimensional Finite Volume discretization}
 \label{fig:fv_scheme}
\end{figure}
In the previous chapter, we derived and characterized the finite difference method. While it is generally simple to apply, it requires the use of structured grids. Suppose now you want to simulate flow over an aircraft or one of its components. Generating structured meshes can become challenging, if not impossible, for complex geometries. Hence, we need methods that are flexible enough such that the geometry is not a fundamental limitation. Instead of evaluating the solution in a pointwise fashion, consider subdividing a generic domain $\Omega$ into volumes or cells $\Omega_i$, as shown in Figure~\ref{fig:fv_scheme}. Within each cell, the value of the function $u(x)$ can be assumed constant and is calculated using an integral approach. This idea leads to the derivation of the finite volume method, which is one of the most commonly used spatial discretizations in the industry. It was initially developed for its application on the general form of the conservation laws, meaning it can be fundamentally applied to solutions that present discontinuities. In this chapter, we present the theory required to apply the finite volume method on scalar as well as systems of conservation laws.

\section{Derivation}
Consider the general conservation law applied to a volume of arbitrary shape $\Omega$, as shown in Figure~\ref{fig:fv_omega}
\begin{figure}[htbp]
 \centering
 \includegraphics[width=0.4\linewidth]{Pictures/ch11_fv_arbitrary_volume}
 \caption{Representation of an arbitrary volume of undefined shape.}
 \label{fig:fv_omega}
\end{figure}
\begin{equation}
 \int_\Omega \frac{\partial \vec u}{\partial t} d \Omega + \int_s \vec F\cdot \hat n ds = 0,
 \label{eq:fv_gcl}
\end{equation}
where $\vec{u}$ is a vector of conserved variables, such as mass, momentum, and energy. $\vec{F}=\vec F(\vec f)$ is the vector of fluxes, $S$ is the area of $\Omega$ with outward unit normal $\hat n$. Recall that for constant volumes, integration and differentiation commute so we can rewrite Equation~\ref{eq:fv_gcl}
\begin{equation}
 \frac{\partial}{\partial t} \int_\Omega \vec{u} d\Omega + \int_s \vec F\cdot \hat n ds = 0.
 \label{eq:fv_gclleibniz}
\end{equation}
Clearly, the integral of the solution represents its total amount within $\Omega$. Hence, the rate of change of the total amount of $\vec{u}$ in a volume is the result of the net flux across the surface $S$ of $\Omega$. As we have stated in the introduction of this chapter, we are looking for averaged values of the solution, specifically the \textit{cell-average} of $\vec u$, which is given by
\begin{equation}
 \vec {\overline{u}} = \frac{ \int_\Omega \vec u d\Omega} {|\Omega|},
 \label{eq:fv_average}
\end{equation}
where $|\Omega|$ is the size of the cell. After averaging the solution, it is clear that it has become constant within each volume, i.e. $\vec {\overline u} \neq \vec {\overline u} (\vec x)$, so it is now only a function of time $\overline {\vec u} = \overline {\vec u} (t)$. If we combine Equations~\ref{eq:fv_gclleibniz} and~\ref{eq:fv_average}, we obtain
\begin{equation}
 \frac{d \vec{\overline u}}{dt} + \frac{1}{|\Omega|} \int_s \vec{F} \cdot\hat nds=0.
\end{equation}
Now, we have an equation for the rate of change of the averaged quantity $\vec{\overline u}$. What remains is to define the value of $\vec{F}$. First, we note that the averaged solution $\vec{\overline u}$ must converge to the instantaneous solution $\vec{u}$ in the limit of sufficiently small volumes
\begin{align}
 \lim_{|\Omega| \rightarrow 0} \vec{\overline u} = \vec u,
\end{align}
and hence,
\begin{equation}
 \lim_{|\Omega| \rightarrow 0} \vec F (\vec {\overline{u}}) = \vec F(\vec u).
\end{equation}
This leads to the general form of the finite volume method, which we write 
\begin{eqBox}
\begin{equation}
 \frac{d \vec{\overline u}}{dt} = - \frac{1}{|\Omega|} \int_s \vec{F} (\vec{\overline u})\cdot\hat nds=0,
 \label{eq:fvm}
\end{equation}
\end{eqBox}
where $d/dt$ can be solved using an appropriate temporal discretization (see Chapter~\ref{ch:timestepping}). 

Generally, we can make relatively good approximations of arbitrary volumes using straight-sided elements such as triangles and tetrahedra. Consider the triangular volume $\Omega_i$ displayed in Figure~\ref{fig:fv_straightsided}. The solution $\vec{\overline u}$ is constant within the triangular shape. In addition, each of the faces has an area of $S_1$, $S_2$ and $S_3$. Observe that since the sides of the considered volume are straight, the flux vectors are constant across each of the faces. Hence, the surface integral in Equation~\ref{eq:fvm} can be trivially solved via summation. For the $i$-th volume, the general form of the FV method for straight grids can be written
\begin{figure}[htbp]
 \centering
 \includegraphics[width=0.3\linewidth]{Pictures/ch11_fv_cellavg}
 \caption{Representation of solution, fluxes and faces of a straight-sided element.}
 \label{fig:fv_straightsided}
\end{figure}
\begin{eqBox}
 \begin{equation}
 \frac{d \overline{\vec u}_i}{dt} = - \frac{1}{|\Omega_i|} \sum_{j=1}^{m} \vec F_j \cdot \hat n_j S_j,
 \label{eq:fv_straightsided}
 \end{equation}
\end{eqBox}
where $m$ is the number of faces, $F_i$ is the flux across the $j$-th face with outward unit normal $\hat n_j$ and area surface $S_j$.

\section{The Riemann Problem}
\begin{figure}[htbp]
 \centering
 \includegraphics[width=0.4\linewidth]{Pictures/ch11_fv_riemann_diagram}
 \caption{Representation of the Riemann problem}
 \label{fig:fv_riemannproblem}
\end{figure}
You may now realize that the solution is piecewise linear, i.e., we have approximated a function $\vec u(x,t)$ by constant values $\vec {\overline u}_i(t)$ at each volume $\Omega_i$. Usually, in finite difference methods, the value of the flux can be simply computed as $\vec{F}=\vec{F}(\vec{\overline{u}})$ due to the pointwise approach. In contrast, consider the two volumes shown in Figure~\ref{fig:fv_riemannproblem} as a result of a finite volume discretization. We are looking for the value of the fluxes across each of the volume's faces to solve Equation~\ref{eq:fv_straightsided}. However, note that on each side of the interface, we have two values of the solution $\vec{\overline{u}}_L,~\vec{\overline{u}}_R$, which are expected to yield a single value of the flux. Hence, the flux at the interface is
\begin{eqBox}
 \begin{equation}
 \vec{F} = \vec{F} (\vec{\overline{u}}_L,~\vec{\overline{u}}_R).
 \end{equation}
\end{eqBox}
This is known as the \textit{Riemann problem}. The solution to the Riemann problem depends on the physics of the problem. For hyperbolic equations, it is said to be a similarity solution and is related to the \textit{characteristic speed} of PDE. We now present some examples of the Riemann problem for scalar conservation laws and the resulting finite-volume discretizations. Later in this chapter, we will come back to this topic and derive some solutions for systems of hyperbolic equations.

\section{Example Applications}
We have now derived the finite volume method and described some of its characteristics. We have introduced a consequence of the discretization, which is the Riemann problem. In this section, we present finite-volume schemes applied to our simple one-dimensional scalar conservation laws, as well as solutions to the Riemann problem for each of these equations.
\subsection{Linear Advection}
Consider the general conservation law applied to the one-dimensional advection equation. For scalar conservation laws, we may write
\begin{equation}
 \vec{u} = u,
\end{equation}
and hence the advection flux is given by
\begin{equation}
 \vec{F} = f = \alpha u,
\end{equation}
where $\alpha$ is the advection velocity. A finite-volume discretization for this problem can be seen in Figure~\ref{fig:fv_advection}, where $\alpha >0$. Consider the cell $\Omega_i$. We need to find the value of the flux at each of the faces of the cell. 
\begin{figure}[htbp]
 \centering
 \includegraphics[width=0.5\linewidth]{Pictures/ch11_fv_advection}
 \caption{Finite-volume linear advection.}
 \label{fig:fv_advection}
\end{figure}
Different approaches can be considered, which result in distinct finite-volume schemes, each with its own characteristic error and order of accuracy. We now present three options to compute the Riemann flux and the resulting discretizations.
\subsubsection{Upwind}
We have previously assumed $\alpha>0$, which means information travels from left to right. A natural choice of Riemann flux is \textit{upwind}. Here, the value of the flux is computed from the left-hand-side solution value at each interface. Hence, 
\begin{equation}
 \vec F(\overline u_L, \overline u_R) = \alpha \overline u_L, 
\end{equation}
which can be written 
\begin{align}
 \vec F_1 &= \alpha \overline u_{i-1}, \\ 
 \vec F_2 &= \alpha \overline u_{i},
\end{align}
for the left and right faces of $\Omega_i$, respectively. Recall the finite-volume discretization for straight-sided elements in Equation~\ref{eq:fv_straightsided}, which we now rewrite for this problem considering $\hat n_1=-1$, $\hat n_2=1$ and $S_1=S_2=1$
\begin{equation}
 \frac{d \overline{u}_i}{dt} = -\frac{1}{\Delta x} \left[\alpha \overline {u}_i - \alpha \overline u_{i-1}\right],
\end{equation}
or simply
\begin{eqBox}
\begin{equation}
 \frac{d \overline{u}_i}{dt} = -\alpha \frac{ \overline {u}_i - \overline u_{i-1}}{\Delta x},
\end{equation}
\end{eqBox}
which is identical to the original first-order finite-difference method. 
\subsubsection{Central}
A second choice of Riemann flux results from an averaged value of the solution at each interface. This means
\begin{equation}
 \vec F(\overline u_L, \overline u_R) = \frac{1}{2}\left[\alpha \overline u_L + \alpha \overline u_R \right].
\end{equation}
Hence, the fluxes can be written
\begin{align}
 \vec F_1 = \frac{1}{2}\left[\alpha \overline u_{i-1} + \alpha \overline u_{i} \right],\\
 \vec F_2 = \frac{1}{2}\left[\alpha \overline u_{i} + \alpha \overline u_{i+1} \right],
\end{align}
for the left and right interface of $\Omega_i$, respectively. By substituting these fluxes into Equation~\ref{eq:fv_straightsided}, a central finite-volume scheme can be written
\begin{eqBox}
\begin{equation}
 \frac{d\overline u_i}{dt} = - \alpha \frac{\overline u_{i+1} - u_{i-1}}{2 \Delta x},
\end{equation}
\end{eqBox}
which is identical to the second-order finite-difference method. 
\subsubsection{Blended}
Consider upwind and central fluxes given by
\begin{align}
 f_u & = \alpha \overline u_L, \\
 f_c & = \frac{\alpha}{2} \left(\overline u_L + \overline u_R \right),
\end{align}
respectively, where $f_u$ yields a \textit{low-resolution} first order scheme and $f_c$ yields a second-order, \textit{high-resolution} scheme. We can combine these two approaches to obtain a \textit{blended} scheme, such that
\begin{eqBox}
\begin{equation}
 f_b = f_u + \phi f_c,
\end{equation}
\end{eqBox}
where $\phi$ is a weighting parameter that can recover the upwind, central or yield a blended scheme, i.e.
\[
 f = 
\begin{cases}
 f_u & \text{if}~\phi = 0, \\
 f_c & \text{if}~\phi = 1, \\ 
 f_b & \text{if}~0 < \phi < 1. 
\end{cases}
\]
We have shown that we can recover finite-difference schemes on structured grids for the linear advection equation considering appropriate choices of Riemann solver. We now explore whether this is also true for the Burgers equation.
\subsection{Burgers Equation}
We now consider the Burgers equation. As we have previously stated, for scalar conservation laws
\begin{equation}
 \vec{u} = u,
\end{equation}
and in the case of Burgers, the flux can be written
\begin{equation}
 \vec{F} = f(u) = \frac{1}{2} u^2.
\end{equation}
Recall that solutions in Burgers equation tend to develop discontinuities at a finite time even if they are initially smooth. Hence, a common Riemann flux choice for this problem is upwind due to its dissipative effect. We can write the fluxes at each face of $\Omega_i$
\begin{align}
 F_1 &= \frac{1}{2} \overline u_{i-1}^2, \\ 
 F_2 &= \frac{1}{2} \overline u_{i}^2.
\end{align}
By following the same procedure as with the advection equation, we obtain the finite-volume scheme from Equation~\ref{eq:fv_straightsided}, 
\begin{equation}
 \frac{d\overline u_i}{dt} = - \frac{1}{\Delta x} \left[\frac{\overline u_i^2}{2} - \frac{\overline u_{i-1}^2}{2}\right],
\end{equation}
which we simplify
\begin{eqBox}
\begin{equation}
 \frac{d\overline u_i}{dt} = - \frac{1}{2 \Delta x} \left[\overline u_i^2 - \overline u_{i-1}^2\right].
 \label{eq:fv_burgers}
\end{equation}
\end{eqBox}
Equation~\ref{eq:fv_burgers} is identical to the first-order upwind scheme for Burgers equation derived using finite differences.
\subsection{Linear Diffusion}
Consider the scalar linear diffusion equation, where
\begin{equation}
 \vec{u} = u,
\end{equation}
and the corresponding flux can be obtained from Fourier's law
\begin{equation}
 \vec{F} = f = -\beta \frac{\partial u}{\partial x},
\end{equation}
where $\beta$ is a constant diffusion coefficient. The resulting finite-volume discretization for this problem can be observed in Figure~\ref{fig:fv_diffusion}. Note that at each interface, we need to compute the derivative of the solution. Due to the behaviour of the diffusion equation, one should expect the Riemann flux to include the effects from both sides of the element.
\begin{figure}[htbp]
 \centering
 \includegraphics[width=0.5\linewidth]{Pictures/ch11_fv_diffusion}
 \caption{Finite-volume linear diffusion.}
 \label{fig:fv_diffusion}
\end{figure}
We can compute the derivatives considering an upwind and a downwind difference approach at the left and right faces , respectively. Hence,
\begin{align}
 \left(\frac{\partial u}{\partial x}\right)_1 & \approx \frac{\overline u_i - \overline u_{i-1}}{\Delta x}, \\
 \left(\frac{\partial u}{\partial x}\right)_2 & \approx \frac{\overline u_{i+1} - \overline u_{i}}{\Delta x}.
\end{align}
Then the respective fluxes can be computed from
\begin{align}
 \vec F_1 &=-\beta \left(\frac{\partial u}{\partial x}\right)_1 \approx -\beta \frac{\overline u_i - \overline u_{i-1}}{\Delta x},\\
 \vec F_2 &=-\beta \left(\frac{\partial u}{\partial x}\right)_2 \approx -\beta \frac{\overline u_{i+1} - \overline u_{i}}{\Delta x}.
\end{align}
By substituting the fluxes into Equation~\ref{eq:fv_straightsided}, we obtain the finite-volume method for linear diffusion
\begin{eqBox}
 \begin{equation}
 \frac{d\overline u_i}{dt} = \beta \frac{\overline u_{i+1}-2\overline u_{i} + \overline u_{i-1}}{\Delta x^2},
	\label{eq:fv_diffusion}
 \end{equation}
\end{eqBox}
which is second-order accurate. The resulting scheme is central, which agrees with the physics of diffusive processes.  Note that Equation~\ref{eq:fv_diffusion} is identical to the second-order finite-difference scheme we previously derived for this problem. 

\section{Linear Hyperbolic Problems} \label{sec:linear_hyperb_problems}
In Chapter 2, we stated that first-order partial differential equations are naturally hyperbolic, meaning that they exhibit wave-like solutions. The simplest case is our advection equation
\begin{equation}
	\frac{\partial u}{\partial t} + \alpha \frac{\partial u}{\partial x} = 0, 
	\label{eq:hyperprob_advection}
\end{equation}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\linewidth]{Pictures/ch11_lsc_advection_dye}
	\caption{Initial and final state of the advection of a dye particle in a one-dimensional flow.}
	\label{fig:advection_dye}
\end{figure}
which can model, for instance, the tracing of a dye particle in a one-dimensional geometry, such as a pipe. Suppose that you want to trace this particle. At time $t=0$, the particle is located at $x=0$ as shown in Figure~\ref{fig:advection_dye}. After some time, we expect the particle to have translated a distance $d=\alpha t$. Hence, it can be easily checked that solutions to Equation~\ref{eq:hyperprob_advection} are given by
\begin{equation}
	u(x,t) = u_0 (x - \alpha t), 
\end{equation}
where $u_0$ is the initial solution. Note that the particle has only changed location, and no shape changes or any diffusion effects are modelled by Equation~\ref{eq:hyperprob_advection}.

Consider now the space-time plane $x\times t$. In this space, we can find \textit{characteristic lines} $x(t)$ along which the solution remains unchanged, that is, lines where $du=0$. We can then rewrite the solution as $u(x(t),t)$, and by using the chain rule, we have 
\begin{equation}
	\frac{d}{dt} (u(x(t),t) = \frac{\partial u}{\partial t} + \frac{d x}{dt} \frac{\partial u}{\partial x} = 0.
	\label{eq:dudt_advection}
\end{equation}
Now, compare Equations~\ref{eq:dudt_advection} and~\ref{eq:hyperprob_advection}. It is clear that
\begin{equation}
	\frac{dx}{dt} = \alpha.
	\label{eq:dxdt_advection}
\end{equation}
Hence the \textit{characteristic speed} of a scalar hyperbolic equation is its advection velocity. From the above relation, we can obtain an equation for the characteristic lines of our PDE, given by
\begin{equation}
	x = x_0 + \alpha t,
\end{equation}
where a curve can be drawn for each initial value of $x_0$ corresponding to $u(x,t=0)=u(x_0)$. 
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\linewidth]{Pictures/ch11_lsc_characteristics}
	\caption{Characteristic lines for the linear advection equation with $\alpha>0$ for different values of $x_0$.}
	\label{fig:linscal_charact}
\end{figure}

Consider $\alpha$ to be positive. The resulting characteristic curves are shown in Figure~\ref{fig:linscal_charact}. Observe that the slope of the curves is $\alpha$. Hence, we can find the value of $u$ at any point $(x,t)$ by tracing curves parallel to those in Figure~\ref{fig:linscal_charact}. We have characterized the behaviour of scalar hyperbolic equations. To analyze systems of equations, we must take into account additional considerations, which we explore in the next section.

\subsection{Linear Hyperbolic Systems}
\label{sec:linear_hyper_systems}
The advective components in our governing equations are responsible for the propagation of information from one point to another. In general, scalar conservation laws are simplifications of more complicated systems. Some of these systems are hyperbolic, which we can analyze as a superposition of advection equations with their corresponding wave speed. A system of $m$ equations
\begin{equation}
\frac{\partial \vec u}{\partial t} + A \frac{\partial \vec u}{\partial x} = 0,\quad \vec u = \begin{bmatrix} u_1 & u_2 & \ldots & u_m\end{bmatrix},
\label{eq:hyperprob_linearsystem}
\end{equation}
is said to be hyperbolic if the constant $m\times m$ matrix $A$, which is generally dense, has $m$ real distinct eigenvalues $\lambda_i$ and $m$ linearly independent eigenvectors $\vec q_i$. This can be written as the eigenvalue problem
\begin{equation}
	A \vec q_i = \lambda_i A,\quad i=1,2,\ldots,m,
\end{equation}
or in matrix form
\begin{equation}
	A Q = Q \Lambda,
	\label{eq:hyperprob_eigen}
\end{equation}
where $Q$ is the matrix of eigenvectors
\begin{equation}
	Q = 
	\begin{bmatrix}
		\vec q_1 & \vec q_2 & \ldots & \vec q_m
	\end{bmatrix}, \quad
\end{equation}
and $\Lambda$ is the diagonal matrix containing the eigenvalues of $A$
\begin{equation}
	\Lambda = 
	\begin{bmatrix}
		\lambda_1 	& 0			& \ldots  & 0 	   \\
		0	 		&  	\ddots  &         & \vdots \\
		\vdots 		&  		    & \ddots  & 0 \\
		0 			& \ldots	& 0		  & \lambda_m
	\end{bmatrix}.
\end{equation}
We have shown in the previous section that the scalar linear equation simply propagates information at a given characteristic speed, which represents the slope of the characteristic curves $x(t)$ in a space-time plane. In the case of a linear hyperbolic system, more than a single wave speed is embedded in the equations. Hence, multiple slopes are associated with any given $(x,t)$ point in the $x\times t$ plane, which are given by the eigenvalues of $A$. Now, how are these characteristic speeds related to $\vec u$? It turns out that the solution is considered to be a superposition of these waves. To illustrate this, the original system needs to be decoupled into $m$ independent equations. This is done by diagonalizing the constant matrix $A$.

Recall the $m$ eigenvectors of $A$ are linearly independent. Hence, we can transform Equation~\ref{eq:hyperprob_linearsystem} into \textit{characteristic form} by setting
\begin{equation}
	A = Q \Lambda Q^{-1}.
	\label{eq:A_diagonalize}
\end{equation}
Now, substituting A in Equation~\ref{eq:hyperprob_linearsystem} using~\ref{eq:A_diagonalize} and multiplying by $Q^{-1}$ yields
\begin{equation}
	Q^{-1} \frac{\partial \vec  u}{\partial t} + Q^{-1} Q \Lambda Q^{-1} \frac{\partial \vec u}{\partial t} = 0.
\end{equation}
For simplicity, define the vector of \textit{characteristic variables} $\vec \omega=Q^{-1} \vec{u}$. Hence, the characteristic form of hyperbolic system of PDEs is
\begin{eqBox}
\begin{equation}
	\frac{\partial \vec \omega}{\partial t} + \Lambda \frac{\partial \vec \omega}{\partial x} = 0.
\end{equation}
\end{eqBox}
We have now decoupled our original system into $m$ independent advection equations of the form
\begin{equation}
	\frac{\partial \omega_i}{\partial t} + \lambda_i \frac{\partial \omega_i}{\partial x} = 0,\quad i=1,\ldots,m,
\end{equation}
each of which has solution
\begin{equation}
	\omega_i = \omega_i^0(x-\lambda_i t),
\end{equation}
where the initial characteristic state can be found directly from the initial conditions by
\begin{equation}
	\vec{\omega}_0 = Q^{-1} \vec u_0. 
	\label{eq:initialstatetocharact}
\end{equation}
Similar to the scalar case, we can now draw characteristic curves in the $x\times t$ plane. These can be observed in Figure~\ref{fig:linsys_charact}, where we have considered the eigenvalues to be sorted 
\begin{equation}
	\lambda_1 < \lambda_2 < \ldots < \lambda_m.
\end{equation}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\linewidth]{Pictures/ch11_lsys_characteristics}
	\caption{Characteristic lines for a linear hyperbolic system of equations.}
	\label{fig:linsys_charact}
\end{figure}
We have now decomposed the vector of conserved quantities $\vec u$ into characteristic variables $\vec \omega$ employing the eigenvectors $\vec q_i$. Hence, the original variables can be recovered by coefficients $\omega_i$ and their corresponding eigenvector.
\begin{equation}
	u(x,t) = \sum_{i=1}^{m} \omega_i(x,t) \vec q_i=\sum_{i=1}^m \omega^0_i(x-\lambda_i t) \vec q_i.
	\label{eq:linsuperposition}
\end{equation}
\subsection{The Riemann Problem}
\subsubsection{The Scalar Case}
Consider the following initial-value problem (Figure~\ref{fig:scalar_riemann_u0}) for the scalar advection equation 
\begin{align}
	\frac{\partial u}{\partial t} + \alpha \frac{\partial u}{\partial x} = 0, \quad u(x,0) = 
	\begin{cases}
		u_L & \text{if}~x<0,\\
		u_R & \text{if}~x>0,\\
	\end{cases}
\end{align}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\linewidth]{Pictures/ch11_riem_lsc_u0}
	\caption{Scalar initial condition with discontinuity}
	\label{fig:scalar_riemann_u0}
\end{figure}
where $u_L$ and $u_R$ are piecewise constants and $u_L\neq u_R$. Based on our previous discussions, these constant values simply propagate along some characteristic curve in the $x\times t$ plane along with the discontinuity initially located at $x=0$. The characteristic line with the origin at this point defines the path of the discontinuity with equation $x=\alpha t$, as shown in Figure~\ref{fig:riem_charact_scalar}. Hence, on the left side of this curve, specifically where $x<\alpha t$, the solution is simply $u_L$, and on the right-hand side, where $x>\alpha t$, the solution is $u_R$. Hence, the Riemann solution to the scalar problem is simply
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\linewidth]{Pictures/ch11_riem_lsc_characteristics}
	\caption{Chacteristic line along which discontinuity propagates}
	\label{fig:riem_charact_scalar}
\end{figure}
\begin{eqBox}
\begin{equation}
	u(x,t) = 
	\begin{cases}
		u_L & \text{if}~x<\alpha t,\\
		u_R & \text{if}~x>\alpha t.\\
	\end{cases}
\end{equation}
\end{eqBox}
\subsubsection{Hyperbolic Systems}
Now consider the following initial-value problem, this time for a system of $m$ hyperbolic equations, where multiple values of the solution exist on each side of $x=0$
\begin{align}
	\frac{\partial \vec u}{\partial t} +A \frac{\partial \vec u}{\partial x} = 0, \quad \vec u(x,0) = 
	\begin{cases}
		\vec u_L & \text{if}~x<0,\\
		\vec u_R & \text{if}~x>0.\\
	\end{cases}
	\label{eq:ivp_linearsys}
\end{align}
Recall from Section~\ref{sec:linear_hyper_systems} that linear hyperbolic systems are driven by a superposition of multiple waves. To reveal these underlying advection equations, we must transform the original equations into characteristic form. From Equation~\ref{eq:initialstatetocharact}, we may write
\begin{equation}
	\vec \omega(x,0) = 
	\begin{cases}
		\vec \omega_L & \text{if}~x<0,\\
		\vec \omega_R & \text{if}~x>0,\\
	\end{cases}
	\label{eq:ivp_linearsys}
\end{equation}
where the $i$-th equation has solution
\begin{equation}
	\omega_i(x,t) = 
	\begin{cases}
		\omega_{L,i} &\text{if}~x<\lambda_i t, \\
		\omega_{R,i} &\text{if}~x>\lambda_i t. \\
	\end{cases}
\end{equation}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\linewidth]{Pictures/ch11_riem_lsys_characteristics}
	\caption{Characteristic curves for the Riemann problem with $m=4$. Note the value of the maximum sub-index changes for each point $P$. For $P_{m,L}^*$, $I=1$, for $P_{m}$, $I=2$ and for $P_{m,R}^*$, $I=3$. (Adapted from~\cite{leveque})}
	\label{fig:characteristics_riemann}
\end{figure}
The eigenvalues of $A$ can be observed in Figure~\ref{fig:characteristics_riemann} for the case of $m=4$ equations. Now, suppose you have a set of coordinates $(x,t)$ where you want to know the value of the solution. At any point in the $x\times t$ domain, the solution only depends on $u(x,0)=u(x_0)=u_0$. Hence, we can determine the value of $\vec u$ at any point by drawing lines parallel to the characteristic curves. On the left of $\lambda_1$, tracing parallel lines only yields initial values which correspond to the left state $\vec u_L$, hence the solution can be computed by
\begin{equation}
	\vec u_L = \sum_{i=1}^m  \omega_{L,i} \vec q_i.
\end{equation}
Similarly, at any point on the right side of $\lambda_m$ ($\lambda_4$ in Figure~\ref{fig:characteristics_riemann}) we find
\begin{equation}
	\vec u_R = \sum_{i=1}^m  \omega_{R,i} \vec q_i.
\end{equation}
Now, what if we are interested in finding the solution at the point $P_m$ in Figure~\ref{fig:characteristics_riemann}?. Tracking the solution back to its initial value shows that values of $x-\lambda t$ are located on both the left and right-hand sides of $x=0$. Hence, we require a combination of both left and right states. Note that at any point within the region defined by $\lambda_2$ and $\lambda_3$, the solution $\vec u_m$ depends on the linear combination of left and right states. Moving $P_m$ to the left or right of these characteristic lines, for example where points $P_{m,L}^*$ and $P_{m,R}^*$ are will require a different combination $\vec \omega_L$ and $\vec\omega_R$. Hence, to generalize the equation for the solution at any point, define $I^*=I^*(x,t)$ to be the maximum index $i$ for which $x>\lambda_i t$.  Then, the solution can be found by
\begin{equation}
	\vec u_m = \sum_{i=I^*+1}^m \omega_{L,i} \vec q_i + \sum_{i=1}^{I^*} \omega_{R,i} \vec q_i.
	\label{eq:riemann_solution}
\end{equation}
Since the solution is constant along each characteristic line, the jump between two states is also constant and is given for the whole system
\begin{equation}
	\vec \delta = Q^{-1}(\vec u_R - \vec u_L). 
	\label{eq:lin_rankinehugoniot}
\end{equation}
Hence the jump across the $\lambda_i$-characteristic is given by $\delta_i = \omega_{R,i}-\omega_{L,i}$, which is known as the strength of the $i$-th wave. Note that Equation~\ref{eq:lin_rankinehugoniot} is called the \textit{Rankine-Hugoniot} condition, and will be useful for the analysis of nonlinear problems later in this chapter.
\subsubsection{Example case: Linear Acoustics}
Consider the propagation of sound waves in a motionless gas. Small changes in pressure and density propagate in the surrounding air. This propagation is governed in one dimension by
\begin{equation}
	\begin{bmatrix}
		\rho \\ \rho v 
	\end{bmatrix}_t
	+
	\begin{bmatrix}
		\rho v \\ \rho v^2 + p(\rho)
	\end{bmatrix}_x
	= 0.
	\label{eq:massmomentum_acoustics}
\end{equation}
These are the Euler equations, one of which is the nonlinear PDE for the conservation of momentum. This system can be written in \textit{quasilinear} form
\begin{equation}
	\frac{\partial \vec u}{\partial t} + \vec {f^\prime}(\vec u) \frac{\partial \vec u}{\partial x} = 0,
\end{equation}
where
\begin{align}
	\vec u &= 
	\begin{bmatrix}
		\rho \\ \rho v
	\end{bmatrix}
	=
	\begin{bmatrix}
		u_1 \\ u_2
	\end{bmatrix},
	\\
	\vec f(\vec u) &= 
	\begin{bmatrix}
		\rho v \\ \rho v^2 + p(\rho)
	\end{bmatrix}
	=
	\begin{bmatrix}
		u_2 \\ \frac{u_2^2}{u_1^2} + p(u_1)
	\end{bmatrix}.
\end{align}
Then $A = \vec {f^\prime} (\vec u)$ is the Jacobian matrix and is given by
\begin{equation}
	A = \vec {f^\prime} (\vec u) =
	\begin{bmatrix}
		\frac{\partial f_1}{\partial u_1} & \frac{\partial f_1}{\partial u_2} \\ 
		\frac{\partial f_2}{\partial u_1} & \frac{\partial f_2}{\partial u_2} 
	\end{bmatrix}
	= 
	\begin{bmatrix}
		0 & 1 \\
		-\frac{u_2^2}{u_1^2} + p^\prime(u_1) & \frac{2 u_2}{u_1}
	\end{bmatrix}
	=
	\begin{bmatrix}
		0 & 1 \\
		-v^2 + p^\prime(\rho) & 2 v
	\end{bmatrix}.
\end{equation}
We can analyze systems with small disturbances by linearizing about some state. This can be done by considering the flow properties to be the sum of an initial variable plus a perturbation term
\begin{align}
	u &= \tilde u(x,t), \\
	\rho &= \rho_0 + \tilde \rho(x,t), \\
	p & = p_0 + \tilde p(x,t),
\end{align}
and since the gas is considered motionless, we have set $u_0=0$. Neglecting products of small disturbances and applying the chain rule to the momentum equations, the linear system can be written 
\begin{eqBox}
\begin{equation}
	\begin{bmatrix}
		\tilde \rho \\  \tilde u
	\end{bmatrix}_t 
	+ \begin{bmatrix}
		0 & \rho_0 \\
		\frac{c^2}{\rho_0} & 0 \\
	\end{bmatrix}
	\begin{bmatrix}
		\tilde \rho \\  \tilde u
	\end{bmatrix}_x
	= 0,
	\label{eq:linearized_gasdyn}
\end{equation}
\end{eqBox}
where $c=\sqrt{p'(\rho_0)}$ is the speed of sound. Now, consider the following Riemann problem
\begin{equation}
	\vec u(x,0) = \vec u_0 = 
	\begin{cases}
		\vec u_L & \text{if }x<0, \\
		\vec u_R & \text{if }x>0,\\
	\end{cases}
\end{equation}
where $\vec u_L = \begin{bmatrix} \rho_L & u_L \end{bmatrix}^T$ and $\vec u_R = \begin{bmatrix} \rho_R & u_R \end{bmatrix}^T$, $\vec u_L \neq \vec u_R$. Note that the problem is linear due to the above procedure. We can compute the eigenvalues of $A$ in Equation~\ref{eq:linearized_gasdyn} by setting
\begin{equation}
	\det(A-I\lambda) = 0,
\end{equation}
where $I$ is the identity matrix. Solving the eigenvalue problem yields the characteristic polynomial 
\begin{equation}
	\lambda^2 - c^2 = 0,
\end{equation}
which has solutions 
\begin{align}
	\lambda_1 = -c, \quad
	\lambda_2 = +c.
\end{align}
The corresponding matrix of eigenvectors can be found to be
\begin{align}
	Q = 
	\begin{bmatrix}
		\rho_0 & \rho_0 \\
		-c & c
	\end{bmatrix},
\end{align}
with inverse
\begin{align}
	Q^{-1}= 
	\frac{1}{2\rho_0 c}
	\begin{bmatrix}
		c & -\rho_0 \\
		c & \rho_0
	\end{bmatrix}.
\end{align}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\linewidth]{Pictures/ch11_riem_linacoustics_characteristics}
	\caption{Characteristic curves for the linear acoustics example}
	\label{fig:acoustics_charact}
\end{figure}
As we expected, the solution consists of a superposition of left-going and right-going sound waves, which represent the slopes of the characteristic lines shown in Figure~\ref{fig:acoustics_charact}. Now, we analyze the characteristic form of Equation~\ref{eq:linearized_gasdyn} seeking a Riemann solution $\vec u_m$. Note that for systems of $m=2$ equations, the Riemann state only depends on the single region highlighted in Figure~\ref{fig:acoustics_charact}. We start by finding the values of $\vec \omega_L$ and $\vec \omega_R$ by
\begin{equation}
	\vec \omega_L = 
	\begin{bmatrix}
		\omega_{L,1} \\ \omega_{L,2}
	\end{bmatrix}
	= Q^{-1} \vec v_L = \frac{1}{2c\rho_0}
	\begin{bmatrix}
		c & -\rho_0 \\
		c & \rho_0 
	\end{bmatrix}
	\begin{bmatrix}
		\rho_L \\ v_L
	\end{bmatrix}.
\end{equation}
Hence
\begin{align}
	\omega_{L,1} &= \frac{c \rho_L - \rho_0 v_L}{2 c \rho_0}, \\
	\omega_{L,2} &= \frac{c \rho_L + \rho_0 v_L}{2 c \rho_0}.
\end{align}
Similarly, for the right state we can find
\begin{align}
	\omega_{R,1} &= \frac{c \rho_R - \rho_0 v_R}{2 c \rho_0}, \\
	\omega_{R,2} &= \frac{c \rho_R + \rho_0 v_R}{2 c \rho_0}.
\end{align}
In Equation~\ref{eq:riemann_solution}, $I^*=1$, hence
\begin{equation}
	\vec u_m = 
	\begin{bmatrix}
		\rho_m \\ v_m
	\end{bmatrix} =  \sum_{i=2}^2 \omega_{L,i} \vec q_i + \sum_{i=1}^{1} \omega_{R,i} \vec q_i = 
	\omega_{L,2} \vec q_2 + \omega_{R,1} \vec q_1,
\end{equation}
which yields the exact Riemann solution for the linear acoustics equation
\begin{eqBox}
	\begin{align}
		\rho_m &= \frac{1}{2} (\rho_R+\rho_L) + \frac{1}{2}\frac{\rho_0}{c} (u_L-u_R), \\
		u_m &= \frac{1}{2}\frac{c}{\rho_0}(\rho_L-\rho_R) + \frac{1}{2} (u_R+u_L).
		\label{eq:riemann_solution_acoustics}
	\end{align}
\end{eqBox}
Similar to~\cite{toro_riemann}, we can define the values of $\rho_0=1$, $c=1$ and given the initial left and right states
\begin{equation}
	\vec u_L = 
	\begin{bmatrix}
		\rho_L \\ v_L
	\end{bmatrix}
	 = 	
	 \begin{bmatrix}
		0.5 \\ 0
	\end{bmatrix},
	\quad
	\vec u_R = 
	\begin{bmatrix}
		\rho_R \\ v_R
	\end{bmatrix}
	 = 	
	 \begin{bmatrix}
		0.25 \\ 0.0
	\end{bmatrix},
\end{equation}
as shown in Figure~\ref{fig:riem_acoustics_example_u0}, we can determine the middle state from Equation~\ref{eq:riemann_solution_acoustics}. Hence $\rho_m=0.375$ and $v_m=0.125$. Results can be seen in Figure~\ref{fig:riem_acoustics_example_sol}, where a symmetric wave can be seen to have moved from $x=0$ to $-ct$ and $ct$ for the left and right discontinuities, respectively. 
\begin{figure}[htbp]
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\linewidth]{Pictures/ch11_riem_linacoustics_u0}
		\caption{Initial condition}
		\label{fig:riem_acoustics_example_u0}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\linewidth]{Pictures/ch11_riem_linacoustics_sol}
		\caption{Solution at $t>0$}
		\label{fig:riem_acoustics_example_sol}
	\end{subfigure}
	\caption{Example solution with values for the linear acoustics case.}
	\label{fig:riem_acoustics_example}
\end{figure}
\section{Nonlinear Hyperbolic Problems}
Consider the following one-dimensional scalar conservation law
\begin{equation}
	\frac{\partial u}{\partial t} + \frac{\partial f(u)}{\partial x} = 0,
\end{equation}
where $f(u)$ is the flux. In previous sections, we have considered a linear flux $f(u)=\alpha u$, which yields the advection equation. Now, consider a nonlinear flux $f(u)=\frac{1}{2}u^2$. Now, the solution is not expected to simply translate, but also various parts of the solution will deform, leading to the generation of shocks. This is one of our simplified system of equations, which we have referred to as the inviscid Burgers equation. 

Similar to Section~\ref{sec:linear_hyperb_problems}, we want to analyze the characteristic behaviour of this equation. In order to do this, we write the \textit{quasilinear form} of Burgers' equation
\begin{equation}
	\frac{\partial u}{\partial t} + u\frac{\partial u}{\partial x} = 0,
	\label{eq:nonconserv_burgers}
\end{equation}
which we will use to analyze the characteristic curves. Note that $f'(u) = u$. Recall that in the $x\times t$ plane, we look for lines along which the solution does not change. Hence, using the chain rule for $u(x(t), t)$, we can write
\begin{equation}
	\frac{du}{dt} = \frac{\partial u}{\partial t} + \frac{dx}{dt}{\partial t} = 0.
	\label{eq:chainrule_burgers}
\end{equation}
If we compare Equations~\ref{eq:chainrule_burgers} and~\ref{eq:nonconserv_burgers}, it is clear that 
\begin{equation}
	\frac{dx}{dt} = u,
\end{equation}
where the characteristic curve equation is
\begin{equation}
	x = x_0 + ut.
\end{equation}
Since $u$ changes in time, so will the slope of the curves. Clearly, the speed of the solution is the solution itself. Hence, we expect larger values of the solution to travel faster than lower values of the solution. 
\subsection{Nonlinear Hyperbolic Systems}
\subsection{The Riemann Problem}
\subsection{An Exact Riemann Solver for the Euler Equations}
\subsection{Approximate Riemann Solvers}

\chapter{Consistency, Stability, Convergence}
Now that we have derived a few different schemes for solving linear advection, Burgers equation, and linear diffusion, we should now ask ourselves whether they will give us accurate predictions, and are there any restrictions on when they can be used. We have already discussed the importance of order of accuracy, which governs the rate at which the error will converge to zero. Now we will introduce the concepts of consistency, stability, and convergence. If we can prove that our numerical scheme satisfies all three of these properties we can be confident that it is promising approach for the Euler or Navier-Stokes equations. In the current section we will explore these properties in the context of finite difference methods, but the exact same steps can be taken to check whether a finite volume method is suitable or not.

\section{Consistency}
A numerical scheme is consistent if it recovers the exact initial partial differential equation as the grid spacing and time step size are reduced. In other words, the truncation error of the scheme must go to zero in the limit $\Delta x \rightarrow 0$ and $\Delta t \rightarrow 0$. This is usually the case, and it is left as an exercise for the reader to check that our previous finite difference schemes for linear advection, Burgers equation, and linear diffusion satisfy this property. 

However, this is not always the case. To demonstrate this we consider the Dufort-Frankel scheme for linear diffusion
\begin{equation}
u_i^{t+1}  = u_{i}^{t-1} + \frac{2 \beta \Delta t}{\Delta x^2} \left(u_{i-1}^t - u_i^{t+1} - u_i^{t-1} + u_{i+1}^t\right) + \mathcal{O}(\Delta x^2,\Delta t^2,(\Delta t/\Delta x)^2).
\end{equation}
This differs from our example scheme for linear diffusion in that it uses central differences for the time-derivative, and when computing the second-derivative in space it uses the solution at the current grid point at the previous and current time steps. While this scheme may have some useful properties, we note that it has a peculiar term in the truncation error of $\mathcal{O}((\Delta t/\Delta x)^2)$. This can be obtained from a Taylor series expansion of the second derivative. This is concerning, as for any scheme to be consistent with the original partial differential equation we require the truncation error to go to zero. However, if we use a naive approach and simply refine $\Delta x$ and $\Delta t$ at the same rate, this term will not go to zero, and the scheme will not be consistent. For example, if we reduced both the grid spacing and time step size by a half this $\mathcal{O}((\Delta t/\Delta x)^2)$ will remain the same. Hence, in order for the Dufort-Frankel scheme to be consistent with our original partial differential equation we should refine the grid spacing {\it faster} than the time step size. For example, if we reduce $\Delta t$ by a half we should reduce $\Delta x$ by a factor of four. This does not mean the Dufort-Frankel scheme is bad, per-se, but it does mean that care needs to be taken when using it.

\section{Stability}
If we can demonstrate our schemes is consistent, then we know that in the limit $\Delta x \rightarrow 0$ and $\Delta t \rightarrow 0$ we recover the exact partial differential equation. However, this is impossible to achieve in practice as it would require an infinite number of grid points and time steps. When considering stability, we are concerned with whether our numerical scheme will provide {\it physical} solutions when both $\Delta x$ and $\Delta t$ are finite. Lets start by considering what we mean by a physical solution.

In order to advance our solution in time we start with some initial condition. Then, by inserting this initial condition into our scheme we approximate the solution at the next timestep $t+\Delta t$. Then, we insert this approximation back into our scheme to approximate the solution at time $t+2\Delta t$, and this process is repeated over an over again until we reach our final desired time. Hence, the way we advance our simulation in time is effectively a feedback loop, with the output of each time step being recycled back through the numerical scheme to get the solution at each consecutive time step. 

As an analogy we can consider what happens in other simple feedback loops, such as a microphone and speaker. When a performer sings into a microphone their voice is amplified and played bach through the speaker. We expect that this produces a physical replication of their voice, just at a louder volume for the audience. However, if the sound from the speaker is louder than the singers voice at the microphone it will get amplified, played through the speaker at a louder volume, and this cycle then repeats. This results in feedback noise, usually a high-pitched ringing that sounds nothing like the original performer, and usually happens when the performer moves to close to the speaker.

Since our numerical scheme is applied as a feedback loop, the exact same kind of thing can happen. If it amplifies our solution each time step, then the solution will continue to grow, eventually leading to non-physical values such as near-infinite density or pressure. This is colloquially referred to as the solution {\it blowing up}. In contrast, if the scheme damps our solution at each time step it will tend towards physical values, such as the background density or pressure. While this is perhaps not desirable in terms of accuracy, which will be explored later, it is a desirable property in that the solution remains stable and bounded between the initial condition and background state.

Proving stability for non-linear problems, such as Burgers equation, is a relatively daunting task. However, if we restrict ourselves to linear equations, such as linear advection or linear diffusion, then stability can be explored more readily. In order to do this we introduce {\it Von Neumann Analysis}, also commonly referred to as Fourier Analysis. We first assume that our solution $u(x,t)$ can be represented via a Fourier Series, such that
\begin{equation}
	u(x,t) = \sum_{m} b_m(t) e^{i \kappa_m x},
\end{equation}
where $b_m(t)$ is the Fourier coefficients that vary with time as the solution evolves, $\kappa_m$ is a wavenumber, and in this context $i = \sqrt{-1}$. This is the exact same as a conventional Fourer series, with the exception that the coefficients are a function of time describing the time evolution of the system of equations. Furthermore, we take
\begin{equation}
	\kappa_m = \frac{2 \pi m}{2 L}, \: m=0,1,2,\hdots,M,
\end{equation}
where $M$ is chosen based on the maximum wavenumber that can be represented on the grid based on its Nyquist criteria, and $L$ is the length of the domain. Hence, small values of $\kappa_m$ correspond to large waves, and large values of $\kappa_m$ correspond to very compact waves.

Following this approach, we are taking our solution and decomposing it into a number of different waves via a Fourier series. Now, since we have restricted ourselves to linear systems of equations, we can apply the property of superposition. This means that we can analyze each wave independently as a function of time, and the final solution is simply the superposition of all of these waves. This allows us to analyze the behaviour of our numerical scheme for each wavenumber independently, since they are not coupled. Hence, we can write the solution for one particular wave of the Fourier series as
\begin{equation}
	u_m(x,t) = b_m(t) e^{i \kappa_m x},
\end{equation}
where the complete solution is
\begin{equation}
	u(x,t) = \sum_{m} u_m(x,t).
\end{equation}
We will also assume that the time-dependence of the solution is also wavelike with a prescribed frequency in time such that
\begin{eqBox}
\begin{equation}
	u_m(x,t) = e^{at} e^{i \kappa_m x},
\end{equation}
\end{eqBox}
where $a$ is a complex number, referred to as the numerical frequency, that describes how the solution evolves in time. In order to justify this assumption we can consider the linear advection equation applied to an arbitrary wavenumber $\kappa_m$. We note that this wave will propagate at velocity $\alpha$ from left to right. Now if we consider some fixed point in space, denoted by $u_1{t}$, we note that the value of the solution in time will alose behave like a sine wave. Hence, at a fixed point in time, the solution has a wavelike structure in space and, at a fixed point in space, the solution has a wavelike structure in time. Hence, the dual wavelike structure taken for $u_m(x,t)$.

With the wavelike structure of the solution described, we can now explore how that wave will change with time. Of primary importance, at least in terms of stability, is to determine whether the wave will be amplified or damped as the solution evolves. Our objective in this section is to determine whether, and under what conditions, numerical schemes satisfy this stability condition. We note that the solution at time $t+\Delta t$ is simply
\begin{equation}
	u_m(x,t+\Delta t) = e^{a(t+\Delta t)} e^{i \kappa_m x},
\end{equation}
which can be re-written as
\begin{equation}
	u_m(x,t+\Delta t) = e^{at}e^{a\Delta t} e^{i \kappa_m x},
\end{equation}
and, in order for the magnitude of the solution to not be amplified at the next time step, we have the stability constraint
\begin{eqBox}
\begin{equation}
	|e^{a\Delta t}| \leq 1,
\end{equation}
\end{eqBox}
which describes a unit circle in the complex plane. Referred to as the amplification factor, we will next determine under what conditions our numerical schemes satisfy this stability constraint.

\subsection{Explicit Linear Advection}
For a methodological approach, we will break von Neumann down into a number of steps.

\subsubsection{Step 1: Choose the Discrete Scheme}
The first step in von Neumann analysis is to determine what scheme we are interested in analyzing. In this case we will consider our simple first-order scheme for linear advection
\begin{equation}
	\frac{u_i^{t+1} - u_{i}^t}{\Delta t} +  \alpha \frac{u_i^t - u_{i-1}^t}{\Delta x} = 0.
\end{equation}

\subsubsection{Step 2: Apply the Wavelike Solution}
Since we know the prescribed wave-like form of the solution, the grid spacing $\Delta x$, and the time step size $\Delta t$, we can write expressions for the solution for a particular wavenumber $\kappa_m$ at each grid point and time level as
\begin{equation}
	u_{i}^t = e^{at} e^{i \kappa_m x},
\end{equation}
\begin{equation}
	u_{i}^{t+1} = e^{a(t+\Delta t)} e^{i \kappa_m x},
\end{equation}
\begin{equation}
	u_{i-1}^t = e^{at} e^{i \kappa_m (x-\Delta x)}.
\end{equation}
Substituting these into our finite difference approximation yields
\begin{equation}
	\frac{e^{a(t+\Delta t)} e^{i \kappa_m x} - e^{at} e^{i \kappa_m x}}{\Delta t} +  \alpha \frac{e^{at} e^{i \kappa_m x} - e^{at} e^{i \kappa_m (x-\Delta x)}}{\Delta x} = 0.
\end{equation}

\subsubsection{Step 3: Solve for the Amplification Factor}
Noting that all of these terms has a common factor of $e^{at} e^{i \kappa_m x}$ we can simply divide through yielding
\begin{equation}
	\frac{e^{a\Delta t} - 1}{\Delta t} +  \alpha \frac{1 - e^{-i \kappa_m \Delta x}}{\Delta x} = 0.
\end{equation}
Rearranging yields
\begin{equation}
	e^{a\Delta t} = 1 - \sigma + \sigma e^{-i \kappa_m \Delta x},
\end{equation}
where
\begin{equation}
	\sigma = \frac{\alpha \Delta t}{\Delta x},
\end{equation}
is the {\it Courant-Friedrichs-Lewy} (CFL) number. Hence, the amplification factor of our first-order linear advection scheme is 
\begin{eqBox}
\begin{equation}
	|e^{a\Delta t}| = |1 - \sigma + \sigma e^{-i \kappa_m \Delta x}|,
\end{equation}
\end{eqBox}
and our scheme will be stable whenever this is contained within the unit circle in the complex plane. We note that this is a function of two parameters, specifically the wavenumber and the CFL number. Hence, we expect that the amount our solution gets amplified/damped each time step will depend on these two parameters.

\subsubsection{Step 4: Determine the Stability Conditions}
Based on these results, we can conclude that the first-order finite difference scheme for linear advection is stable whenever
\begin{eqBox}
\begin{equation}
	0 \leq \sigma \leq 1.
\end{equation}
\end{eqBox}
That is, it is only stable for CFL numbers less than one. Hence, for a given grid spacing and advection velocity, there is a limit on how large the time step can be. This clearly has implications in terms of computational cost, as the smaller the time step is the more steps must be taken to reach a desired final solution time. Schemes of this type are referred to as being {\it conditionally stable}.


\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\linewidth]{Pictures/ch12_explicit_advection}
	\caption{Stability region for the explicit linear advection scheme.}
	\label{fig:explicit_advection}
\end{figure}

\subsection{Implicit Linear Advection}
In the last section we saw that our first-order finite difference scheme has a stability limit. In this section we will explore a slightly different first-order scheme for linear advection.

\subsubsection{Step 1: Choose the Discrete Scheme}
In this case we use the following finite difference approximation of the linear advection equation
\begin{equation}
	\frac{u_i^{t+1} - u_{i}^t}{\Delta t} +  \alpha \frac{u_i^{t+1} - u_{i-1}^{t+1}}{\Delta x} = 0,
\end{equation}
noting that it is simple the original scheme, but we evaluate the spatial derivative at the next time step rather than the current time step.

\subsubsection{Step 2: Apply the Wavelike Solution}
Our expressions for wavelike solutions at each point are
\begin{equation}
	u_{i}^t = e^{at} e^{i \kappa_m x},
\end{equation}
\begin{equation}
	u_{i}^{t+1} = e^{a(t + \Delta t)} e^{i \kappa_m x},
\end{equation}
\begin{equation}
	u_{i-1}^{t+1} = e^{a(t + \Delta t)} e^{i \kappa_m (x - \Delta x)}.
\end{equation}
Substituting these into our discrete scheme yields
\begin{equation}
	\frac{e^{a(t + \Delta t)} e^{i \kappa_m x} - e^{at} e^{i \kappa_m x}}{\Delta t} +  \alpha \frac{e^{a(t + \Delta t)} e^{i \kappa_m x} - e^{a(t + \Delta t)} e^{i \kappa_m (x - \Delta x)}}{\Delta x} = 0.
\end{equation}

\subsubsection{Step 3: Solve for the Amplification Factor}
Again we note a common factor of $e^{at} e^{i \kappa_m x}$ in all terms, allowing us to divide through yielding
\begin{equation}
	\frac{e^{a\Delta t} - 1}{\Delta t} +  \alpha \frac{e^{a\Delta t} - e^{a\Delta t} e^{-i \kappa_m \Delta x}}{\Delta x} = 0.
\end{equation}
Rearranging yields
\begin{equation}
	e^{a\Delta t} = \frac{1}{1 + \sigma \left( 1 - e^{-i \kappa_m \Delta x} \right)},
\end{equation}
where $\sigma$ is again the CFL number. Hence, the amplification factor is
\begin{eqBox}
\begin{equation}
	|e^{a\Delta t}| = \left| \frac{1}{1 + \sigma \left( 1 - e^{-i \kappa_m \Delta x} \right)} \right|.
\end{equation}
\end{eqBox}

\subsubsection{Step 4: Determine the Stability Conditions}
Based on these results, we can conclude that this finite difference scheme for the linear advection equation is stable provided
\begin{eqBox}
\begin{equation}
	0 \leq \sigma \leq \infty.
\end{equation}
\end{eqBox}
Hence, we are able to take arbitrarily large time steps and maintain stability using this approach. Schemes of this type are referred to as being {\it unconditionally stable}.

\begin{remark}
Although this scheme is unconditionally stable, making at appealing since it allows for arbitrarily large time-steps, it also becomes more difficult/expensive for each time-step. This will be explored in the forthcoming time stepping section.
\end{remark}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\linewidth]{Pictures/ch12_implicit_advection}
	\caption{Stability region for the implicit linear advection scheme.}
	\label{fig:implicit_advection}
\end{figure}

\subsection{Explicit Linear Diffusion}
Similar to the linear advection equation, we can also use von Neumann analysis to analyze the stability of the linear diffusion equation. 

\subsubsection{Step 1: Choose the Discrete Scheme}
We will start with the example scheme we derived in the finite difference section
\begin{equation}
  \frac{u_i^{t+1} - u_{i}^t}{\Delta t} - \beta \frac{u_{i-1}^t - 2u_i^t + u_{i+1}^t}{\Delta x^2} = 0.
\end{equation}

\subsubsection{Step 2: Apply the Wavelike Solution}
Our expressions for wavelike solutions at each point are
\begin{equation}
	u_{i}^t = e^{at} e^{i \kappa_m x},
\end{equation}
\begin{equation}
	u_{i}^{t+1} = e^{a(t + \Delta t)} e^{i \kappa_m x},
\end{equation}
\begin{equation}
	u_{i-1}^{t} = e^{at} e^{i \kappa_m (x - \Delta x)},
\end{equation}
\begin{equation}
	u_{i+1}^{t} = e^{at} e^{i \kappa_m (x + \Delta x)}.
\end{equation}
Substituting these into our discrete scheme yields
\begin{equation}
  \frac{e^{a(t + \Delta t)} e^{i \kappa_m x} - e^{at} e^{i \kappa_m x}}{\Delta t} -  \beta \frac{e^{at} e^{i \kappa_m (x - \Delta x)} - 2e^{at} e^{i \kappa_m x} + e^{at} e^{i \kappa_m (x + \Delta x)}}{\Delta x ^2} = 0.
\end{equation}

\subsubsection{Step 3: Solve for the Amplification Factor}
Again we note a common factor of $e^{at} e^{i \kappa_m x}$ in all terms, allowing us to divide through yielding
\begin{equation}
  \frac{e^{a\Delta t} - 1}{\Delta t} -  \beta \frac{e^{-i \kappa_m \Delta x} - 2 + e^{i \kappa_m \Delta x}}{\Delta x ^2} = 0.
\end{equation}
Rearranging yields
\begin{equation}
	e^{a\Delta t} = 1 + r \left( e^{-i \kappa_m \Delta x} - 2 + e^{i \kappa_m \Delta x} \right), 
\end{equation}
where
\begin{equation}
	r = \frac{\beta \Delta t}{\Delta x^2},
\end{equation}
is similar to the CFL number but for diffusion rather than advection. Finally, the amplification factor is
\begin{eqBox}
\begin{equation}
	|e^{a\Delta t}| = \left| 1 + r \left( e^{-i \kappa_m \Delta x} - 2 + e^{i \kappa_m \Delta x} \right) \right|.
\end{equation}
\end{eqBox}

\subsubsection{Step 4: Determine the Stability Conditions}
Based on this amplification factor, we demonstrate graphically that this scheme will be stable provided
\begin{eqBox}
\begin{equation}
	0 \leq r \leq \frac{1}{2}.
\end{equation}
\end{eqBox}
We note that this scheme is {\it conditionally stable}, similar to the first linear advection scheme we considered. This means if the grid is refined then the time step size must be reduced accordingly to maintain stability. However, we note that the factor of $\Delta x^2$ in $r$ will require the time step size to be reduced with the square of the grid spacing, which can become expensive on finer meshes.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\linewidth]{Pictures/ch12_explicit_diffusion}
	\caption{Stability region for the explicit linear diffusion scheme.}
	\label{fig:explicit_diffusion}
\end{figure}

\begin{remark}
The Navier-Stokes equations have both advective and diffusive terms. Hence, the time step is usually limited by the stricter of the stability constraints of the advective and diffusive schemes that are being used.
\end{remark}

\subsection{Implicit Linear Diffusion}
Similar to the modified linear advection scheme, we can also modify our initial linear diffusion scheme by evaluating the spatial operator at the future unknown solution time.

\subsubsection{Step 1: Choose the Discrete Scheme}
This yields the following scheme
\begin{equation}
  \frac{u_i^{t+1} - u_{i}^t}{\Delta t} - \beta \frac{u_{i-1}^{t+1} - 2u_i^{t+1} + u_{i+1}^{t+1}}{\Delta x^2} = 0.
\end{equation}

\subsubsection{Step 2: Apply the Wavelike Solution}
Our expressions for wavelike solutions at each point are
\begin{equation}
	u_{i}^t = e^{at} e^{i \kappa_m x},
\end{equation}
\begin{equation}
	u_{i}^{t+1} = e^{a(t + \Delta t)} e^{i \kappa_m x},
\end{equation}
\begin{equation}
	u_{i-1}^{t+1} = e^{a(t + \Delta t)} e^{i \kappa_m (x - \Delta x)},
\end{equation}
\begin{equation}
	u_{i+1}^{t+1} = e^{a(t + \Delta t)} e^{i \kappa_m (x + \Delta x)}.
\end{equation}
Substituting these into our discrete scheme yields
\begin{equation}
  \frac{e^{a(t + \Delta t)} e^{i \kappa_m x} - e^{at} e^{i \kappa_m x}}{\Delta t} - \beta \frac{e^{a(t + \Delta t)} e^{i \kappa_m (x - \Delta x)} - 2e^{a(t + \Delta t)} e^{i \kappa_m x} + e^{a(t + \Delta t)} e^{i \kappa_m (x + \Delta x)}}{\Delta x^2} = 0.
\end{equation}

\subsubsection{Step 3: Solve for the Amplification Factor}
Again we note a common factor of $e^{at} e^{i \kappa_m x}$ in all terms, allowing us to divide through yielding
\begin{equation}
  \frac{e^{a\Delta t} - 1}{\Delta t} - \beta \frac{e^{a\Delta t} e^{-i \kappa_m \Delta x} - 2e^{a\Delta t} + e^{a\Delta t} e^{i \kappa_m \Delta x}}{\Delta x^2} = 0.
\end{equation}
Rearranging yields
\begin{equation}
e^{a\Delta t}	= \frac{1}{\left[ 1 - r \left(e^{-i \kappa_m \Delta x} - 2 + e^{i \kappa_m \Delta x} \right)  \right]},
\end{equation}
where $r$ is the same as the original linear diffusion scheme. Finally, the amplification factor is
\begin{eqBox}
\begin{equation}
	|e^{a\Delta t}|	= \left| \frac{1}{\left[ 1 - r \left(e^{-i \kappa_m \Delta x} - 2 + e^{i \kappa_m \Delta x} \right)  \right]} \right|.
\end{equation}
\end{eqBox}

\subsubsection{Step 4: Determine the Stability Conditions}
Based on the form of the amplification factor, we can conclude that this scheme will be stable for
\begin{eqBox}
\begin{equation}
	0 \leq r \leq \infty.
\end{equation}
\end{eqBox}
Hence, this linear diffusion scheme is {\it unconditionally stable}.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\linewidth]{Pictures/ch12_implicit_diffusion}
	\caption{Stability region for the implicit linear diffusion scheme.}
	\label{fig:implicit_diffusion}
\end{figure}

\section{Convergence}
If a scheme is consistent, recovering the exact partial differential equation in the limit $\Delta x \rightarrow 0$ and $\Delta t \rightarrow 0$, and stable, in that the approximate solution does not grow unbounded with time, then Lax's equivalence theorem can be applied. In summary, the theorem states that when given a properly-posed initial value problem, and a numerical scheme that is consistent, stability is the necessary and sufficient condition for convergence. In other words, if we can show that our numerical schemes is consistent and stable, then we can be sure that it will {\it converge} to the true solution of the partial differential equation in the limit $\Delta x \rightarrow 0$ and $\Delta t \rightarrow 0$.

\chapter{Spectral Properties}
In CFD the numerical error introduced by a scheme is typically classified into two general types, dissipation error and dispersion error. In this section we will discuss how to quantify these two types of error, and how they effect different types of solutions.

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[b]{0.49\linewidth}
		\includegraphics[width=\linewidth]{Pictures/ch13_dissipation_diagram}
		\caption{Dissipation}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\linewidth}
		\includegraphics[width=\linewidth]{Pictures/ch13_dispersion_diagram}
		\caption{Dispersion}
	\end{subfigure}
	\caption{Types of numerical error}
	\label{fig:numerical_error}
\end{figure}



\section{Dissipation Error}
We have already discussed dissipation error in the context of stability, stating that in order for a scheme to be stable it must be dissipative or neutrally dissipative. That is, the amplitude of the solution must remain the same or be reduced after each time step. We have shown this via von Neumann analysis to be the amplification factor $|e^{a\Delta t}|$, which was a function of the scheme we are using, and the wavenumber in space $\kappa_m$. While a scheme being dissipative is inherently linked with stability, it also introduces number error. For example, for the linear advection equation we know that the exact solution should have no dissipation, it should be simply the advection of the initial condition with the prescribed advection velocity. However, with our initial finite difference scheme we found that this was not the case, and the amplitude of the waves is decreased as time goes on.

It turns out that all the information we need to understand the dissipation error of the linear advection equation is encoded in the amplification factors we found in the von Neumann analysis section, $|e^{a\Delta t}|$. For example, for the finite difference scheme
\begin{equation}
	\frac{u_i^{t+1} - u_{i}^t}{\Delta t} +  \alpha \frac{u_i^t - u_{i-1}^t}{\Delta x} = 0.
\end{equation}
we found that the amplification factor was 
\begin{equation}
	|e^{a\Delta t}| = |1 - \sigma + \sigma e^{-i \kappa_m \Delta x}|.
\end{equation} 
Recall that, for a given wavenumber $\kappa_m$ and CFL number $\sigma$ this told us how much the solution will be damped over a time step. Hence, we can plot this for a range of permissible wavenumbers in the range $\kappa_m \Delta x \in [0,\pi]$ and CFL numbers within the stability limit $\sigma \in [0,1]$. A plot of this is shown in Figure \ref{fig:dissipation_advection_explicit}. We can observe a few interesting features that define the dissipation error of this particular scheme. First, we note that when the wave size is large relative to the grid spacing, the wave is well resolved regardless of the CFL number. However, as we move to larger wavenumbers there is significant numerical dissipation. Also, the amount of numerical in this region dissipation depends on the CFL number, with larger CFL numbers corresponding to less dissipation. Finally, when the CFL number approaches unity the numerical dissipation approaches zero for all wavenumbers. Hence, when this CFL number is used we recover the exact dissipation relation for linear advection, that the waves do not get damped. The dissipation error of this scheme becomes particularly important in the context of turbulent flows. In this case, large scale structures in the turbulent flow have small wavenumbers relative to the grid spacing. This means that our large scale features will be simulated with relative accuracy. However, small scale turbulent structures that are of a size proportional to the grid spacing have high relative wavenumbers. This means that these fine scale structures will be heavily dissipated by this first order scheme.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\linewidth]{Pictures/ch13_dissipation_adv_explct}
	\caption{Amplification factor vs wavenumber for the explicit advection scheme}
	\label{fig:dissipation_advection_explicit}
\end{figure}
Lets look at a second example, our implicit finite difference scheme
\begin{equation}
	\frac{u_i^{t+1} - u_{i}^t}{\Delta t} +  \alpha \frac{u_i^{t+1} - u_{i-1}^{t+1}}{\Delta x} = 0,
\end{equation}
that had the amplification factor
\begin{equation}
	|e^{a\Delta t}| = \left| \frac{1}{1 + \sigma \left( 1 - e^{-i \kappa_m \Delta x} \right)} \right|.
\end{equation}
We can generate the exact same type of plot for permissible wave numbers $\kappa_m \Delta x \in [0,\pi]$ and, since this scheme is unconditionally stable, we will look at a few CFL numbers in the range $\sigma \in [0,10]$, as shown in Figure \ref{fig:dissipation_advection_implicit}. We note for this scheme, similar to the explicit scheme, that when the wave size is large relative to the grid spacing it is well resolved regardless of the CFL number. However, there is again a strong dependence of the numerical dissipation on the CFL number for high wavenumbers. As the wavenumber gets larger or as the CFL number gets larger the amount of numerical dissipation increases rapidly. Hence, while this scheme was found to be unconditionally stable, using large CFL numbers introduces significant numerical dissipation to all but the largest structures in the flow. Hence, this scheme is typically only used for solving steady-state problems that are not a function of time.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\linewidth]{Pictures/ch13_dissipation_adv_implct}
	\caption{Amplification factor vs wavenumber for the implicit advection scheme}
	\label{fig:dissipation_advection_implicit}
\end{figure}

\section{Dispersion Error}
From the linear advection equation we know that the solution should translate at an exact wave speed $\alpha$. However, our numerical schemes will often introduce another error whereby the numerical wave does not move at the correct velocity. As a results, we often observe that the numerical solution is out of phase with the exact solution, which is referred to as dispersion error. Similar to the dissipation error, dispersion error can also be found from our results from von Neumann analysis. Recall that our assumed form of the solution for any isolated wave component of the full solution was
\begin{equation}
	u_m(x,t) = e^{at} e^{i \kappa_m x},
\end{equation}
and recall the expression for the linear advection equation was
\begin{equation}
	\frac{\partial u}{\partial t} +  \alpha \frac{\partial u}{\partial x} = 0.
\end{equation}
If we substitute our wavelike solution we get
\begin{equation}
	ae^{at} e^{i \kappa_m x} + i \alpha \kappa_m e^{at} e^{i \kappa_m x} = 0.
\end{equation}
We note that this will be solved exactly when
\begin{eqBox}
\begin{equation}
	a = -i \alpha \kappa_m.
\end{equation}
\end{eqBox}
which is called the exact dispersion relation. This tells us that the exact frequency in time, which is described by $a$, is proportional to the wave speed and the wavenumber. Intuitively this makes sense as the faster a wave moves the more the solution at a point should move up and down, and the higher the wavenumber the more quickly the wave will move up and down in time as well. However, our numerical scheme will usually not return a value of $a$ that matches this dispersion relation exactly. Hence, for a given wavenumber we will get back a frequency that has some error, implying that the wave is moving at the wrong speed, which is referred to as dispersion error. Over a finite amount of time $\Delta t$, based on the exact dispersion relation we would expect the wave to move a distance
\begin{equation}
	a \Delta t = -i \alpha \kappa_m \Delta t.
\end{equation}

We note that we can also get to our numerical frequency from von Neumann analysis. For example, for the finite difference scheme
\begin{equation}
	\frac{u_i^{t+1} - u_{i}^t}{\Delta t} +  \alpha \frac{u_i^t - u_{i-1}^t}{\Delta x} = 0,
\end{equation}
we found
\begin{equation}
	e^{a\Delta t} = 1 - \sigma + \sigma e^{-i \kappa_m \Delta x},
\end{equation} 
which was the last step before getting the amplification factor. We can start by splitting the exponential term up into its real and imaginary parts
\begin{equation}
	e^{\Re{(a)}\Delta t} e^{\Im{(a)}\Delta t} = 1 - \sigma + \sigma e^{-i \kappa_m \Delta x}.
\end{equation} 
We first note that the real part of this $e^{\Re{(a)}\Delta t}$ is responsible for the amplification/dissipation of the solution, and is what is extracted in the dissipation section. In contrast, the imaginary part $e^{\Im{(a)}\Delta t}$ is responsible for a change in phase of the wave, which moves it in space. We can extract this imaginary term that is responsible for the phase change from
\begin{equation}
	a\Delta t = \ln \left(1 - \sigma + \sigma e^{-i \kappa_m \Delta x} \right),
\end{equation} 
and then extract the imaginary components
\begin{equation}
	\Im(a\Delta t) = \Im{ \left[ \ln \left(1 - \sigma + \sigma e^{-i \kappa_m \Delta x} \right) \right]}.
\end{equation}
In the ideal case this should be identical to the exact dispersion relation, but in practical applications it is typically either slower or faster, resulting in some phase error from the wave moving at an approximate speed. We can extract the relative speed from
\begin{equation}
	\frac{\Im(a\Delta t)}{-i \alpha \kappa_m \Delta t} = \frac{\Im{ \left[ \ln \left(1 - \sigma + \sigma e^{-i \kappa_m \Delta x} \right) \right]}}{-i \alpha \kappa_m \Delta t},
\end{equation}
which tells us the speed of the numerical wave relative to the exact wave speed. When this value is less than 1 the wave moves too slowly, and when this value is greater than 1 the wave moves to fast.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\linewidth]{Pictures/ch13_dispersion_adv_explct}
	\caption{Dispersion curve for the implicit advection scheme}
	\label{fig:dissipation_advection_implicit}
\end{figure}

\chapter{Modified Equation Analysis}
In the previous sections we have demonstrated that our numerical schemes only approximate the exact PDE we are trying to solve. Furthermore, via von Neumann analysis that we can quantify the type of error we will observe as either dissipation or dispersion. Modified equation analysis is another powerful technique that allows us to better understand how and why a particular numerical scheme behaves the way it does. Using modified equation analysis we can show that, while our numerical scheme does not exactly satisfy the PDE we are trying to solve, it does provide an exact solution to a similar PDE. By determining what this similar PDE is, we can gain further insight into the behaviour of our scheme. In this section we will demonstrate by example how to perform modified equation analysis in the context of linear advection.

\section{Linear Advection}
He we will derive the modified equation for our first order linear advection scheme
\begin{equation}
	\frac{u_i^{t+1} - u_{i}^t}{\Delta t} +  \alpha \frac{u_i^{t} - u_{i-1}^{t}}{\Delta x} = 0.
\end{equation}
In order to derive the modified equation we will start by re-stating our Taylor-Series expansions for the solution at each grid point and time level, dropping the evaluated at notation for compactness and expanding up to the second-order terms, as
\begin{equation}
	u_{i-1}^t = u_i^t - \Delta x \frac{\partial u}{\partial x} + \frac{\Delta x^2}{2} \frac{\partial^2 u}{\partial x^2} + \mathcal{O}(\Delta x^3),
\end{equation}
and using a similar Taylor-Series in time
\begin{equation}
	u_{i}^{t+1} = u_i^t - \Delta t \frac{\partial u}{\partial t} + \frac{\Delta t^2}{2} \frac{\partial^2 u}{\partial t^2} + \mathcal{O}(\Delta t^3).
\end{equation}
We will now insert these expansions into our numerical scheme for their respective terms
\begin{equation}
	\frac{u_i^t - \Delta t \frac{\partial u}{\partial t} + \frac{\Delta t^2}{2} \frac{\partial^2 u}{\partial t^2} + \mathcal{O}(\Delta t^3) - u_{i}^t}{\Delta t} +  \alpha \frac{u_i^{t} - u_i^t - \Delta x \frac{\partial u}{\partial x} + \frac{\Delta x^2}{2} \frac{\partial^2 u}{\partial x^2} + \mathcal{O}(\Delta x^3)}{\Delta x} = 0.
\end{equation}
Simplifying this expression down a bit yields
\begin{equation}
	- \frac{\partial u}{\partial t} + \frac{\Delta t}{2} \frac{\partial^2 u}{\partial t^2} + \mathcal{O}(\Delta t^2) +  \alpha \left[ - \frac{\partial u}{\partial x} + \frac{\Delta x}{2} \frac{\partial^2 u}{\partial x^2} + \mathcal{O}(\Delta x^2) \right] = 0,
\end{equation}
which can then be rearranged to
\begin{equation}
	\label{eqn:38fg93la}
	\frac{\partial u}{\partial t} + \alpha \frac{\partial u}{\partial x} = -\frac{\Delta t}{2} \frac{\partial^2 u}{\partial t^2} + \alpha \frac{\Delta x}{2} \frac{\partial^2 u}{\partial x^2} + \mathcal{O}(\Delta x^2,\Delta t^2).
\end{equation}
We can recognize the right hand side of this equation as the truncation error of our scheme and we note that in the limit as $\Delta t$ and $\Delta x$ go to zero this will converge to the exact PDE. Furthermore, since the behaviour of the temporal derivative on the left hand size is somewhat unclear, we will convert it to a spatial derivative. Starting by differentiating the above expression in time we get
\begin{equation}
	\label{eqn:pelfhv72}
	\frac{\partial^2 u}{\partial t^2} + \alpha \frac{\partial^2 u}{\partial x \partial t} = -\frac{\Delta t}{2} \frac{\partial^3 u}{\partial t^3} + \alpha \frac{\Delta x}{2} \frac{\partial^3 u}{\partial x^2 \partial t} + \mathcal{O}(\Delta x^2,\Delta t^2),
\end{equation}
and in space we get
\begin{equation}
	\label{eqn:dhwis937}
	\frac{\partial^2 u}{\partial t \partial x} + \alpha \frac{\partial^2 u}{\partial x^2} = -\frac{\Delta t}{2} \frac{\partial^3 u}{\partial t^2 \partial x} + \alpha \frac{\Delta x}{2} \frac{\partial^3 u}{\partial x^3} + \mathcal{O}(\Delta x^2,\Delta t^2).
\end{equation}
Rearranging and substituting Equation \ref{eqn:dhwis937} into Equation \ref{eqn:pelfhv72} yields
\begin{equation}
	\frac{\partial^2 u}{\partial t^2} = \alpha^2 \frac{\partial^2 u}{\partial x^2} + \mathcal{O}(\Delta x,\Delta t).
\end{equation}
Hence, if we go back to Equation \ref{eqn:38fg93la} we can replace our second derivative in time with a second derivative in space, yielding
\begin{equation}
	\label{eqn:38fg93la}
	\frac{\partial u}{\partial t} + \alpha \frac{\partial u}{\partial x} = -\frac{\Delta t}{2} \alpha^2 \frac{\partial^2 u}{\partial x^2} + \alpha \frac{\Delta x}{2} \frac{\partial^2 u}{\partial x^2} + \mathcal{O}(\Delta x^2,\Delta t^2,\Delta x\Delta t^2,\Delta t^3).
\end{equation}
Then, using the definition of the CFL number previously defined as $\sigma = \alpha \Delta t / \Delta x$
\begin{equation}
	\frac{\partial u}{\partial t} + \alpha \frac{\partial u}{\partial x} = \frac{\alpha \Delta x}{2}(1-\sigma) \frac{\partial^2 u}{\partial x^2} + \mathcal{O}(\Delta x^2,\Delta t^2,\Delta x\Delta t^2,\Delta t^3),
\end{equation}
and rearranging finally yields
\begin{eqBox}
\begin{equation}
	\label{eqn:shrj986g}
	\frac{\partial u}{\partial t} + \alpha \frac{\partial u}{\partial x} - \frac{\alpha \Delta x}{2}(1-\sigma) \frac{\partial^2 u}{\partial x^2} = \mathcal{O}(\Delta x^2,\Delta t^2,\Delta x\Delta t^2,\Delta t^3).
\end{equation}
\end{eqBox}

If we look at the form of the Equation \ref{eqn:shrj986g} we see that when $\Delta x$ and $\Delta t$ are small the right hand side reduces to zero, and we are left with a general {\it advection-diffusion} equation. Hence, we have shown that our finite difference scheme is actually the exact solution to an advection-diffusion problem, rather than the linear advection equation we initially intended to solve. Furthermore, we note that the diffusion operator will only be valid for $0 \leq \sigma \leq 1$, after which point the sign of the term will change and it will become a non-physical {\it anti-diffusive} operator. This switch from a physical to non-physical PDE coincides with the stability limits we had originally derived for this scheme using von Neumann analysis. Also, as we would expect, our solutions using this method also behave exactly like an advection diffusion equation since the solution also move, but simultaneously dissipates. Also, for the value $\sigma=1$ this diffusion operator disappears, which is also consistent with our observations and predictions based on von Neumann analysis which showed that the amplification factor $| e^{a \Delta t} | = 1$ for this case.

\section{General Observations}
It is important note that the procedure followed here for the first order linear advection scheme can be repeated for any finite difference scheme of your choice. Starting with your numerical scheme you can replace each term with its Taylor Series expansion and rearrange this to try to get the PDE of interest on the left hand side and truncation error on the right hand side. The leading order terms will tell you the nature of the dominant error in the scheme. If the dominant error has an even-order derivative, such as the scheme above, then one would expect that the scheme is {\it dominantly dissipative}. In contrast, if the dominant error has an odd-order derivative then one would expect that the scheme is {\it dominantly dispersive}. Hence, similar to von Neumann analysis, modified equation analysis is a powerful tool to aid in understanding the general behaviour of a numerical scheme, to elucidate its dissipative and dispersive error properties, and to identify its stability limits.

\chapter{Time-Stepping}
\label{ch:timestepping}
In the previous sections we have always approximated the time derivative using first-order finite differences, assuming
\begin{equation}
	\frac{\partial u}{\partial t} = \frac{u_i^{t+1} - u_{i}^t}{\Delta t} + \mathcal{O}(\Delta t).
\end{equation}
In the von Neumann analysis section we demonstrated that two linear advection schemes and two linear diffusion schemes are either conditionally or unconditionally stable using this approach. However, if we try to use this finite difference approach to get higher-order accurate schemes in time, such as a central in time approach
\begin{equation}
	\frac{\partial u}{\partial t} = \frac{u_i^{t+1} - u_{i}^{t-1}}{2 \Delta t} + \mathcal{O}(\Delta t^2),
\end{equation}
we find it is almost always {\it unstable}. Hence, getting higher than first-order accurate solutions in time requires something other than finite differences. Furthermore, for both of the unconditionally stable schemes in the von Neumann section, we have not yet discussed how to actually advance them in time, and how to get higher-order accuracy in time, while maintaining this unconditional stability. Hence, in this section we will discuss more appropriate discretizations for the time derivative.

\section{Explicit}
Our previous approach for discretizing the time derivative is called a {\it multi-step} scheme, since we are using information from multiple time steps to try to estimate the solution at the next time-step. In this section we will introduce {\it multi-stage} time stepping, specifically the well-known classical Runge-Kutta methods. Rather than use the value of the solution at previous time steps, Runge-Kutta methods compute the solution at intermediate stages between time level $t$ and $t+1$. Then, these intermediate solutions are cleverly combined to achieve a more stable and potentially higher-order accurate solution in time. In all of these cases we will re-arrange our system into a general form
\begin{equation}
	\frac{\partial u}{\partial t} = R(u),
\end{equation}
where $R(u)$ is typically referred to as the {\it residual} or the {\it right hand side}. Note that the term residual is used in several different contexts in CFD.

\subsection{Forward Euler}
The forward Euler, or explicit Euler, scheme uses just one stage to predict the solution at the next time step. In fact, it is equivalent to our previous finite difference approach. In this scheme we get
\begin{equation}
	u^{t+1} = u^t + \Delta t R(u).
\end{equation}
Graphically, this can be interpreted as using the current derivative of the solution in time at time $t$, and simply extrapolating based on this derivative to the time $t+1$. This approach is $\mathcal{O}(\Delta t)$ in time.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\linewidth]{Pictures/ch15_explicit_euler}
	\caption{Explicit Euler}
	\label{fig:explicit_euler_method}
\end{figure}
\subsection{Heun's Method}
Heun's method is a fairly simple but clever improvement on the explicit Euler scheme. We start by using the explicit Euler method to build an approximation of the solution at $t+1$
\begin{equation}
	\tilde{u} = u^t + \Delta t R(u),
\end{equation}
where $\tilde{u}$ is an initial guess for the solution. Then, we use an average of the following right hand sides to compute the solution at the next time step
\begin{equation}
	\tilde{u}^{t+1} = u^t + \Delta t \left[\frac{R(u) + R(\tilde{u})}{2} \right].
\end{equation}
This approach is called a two-stage scheme, since we had to compute our solution at one intermediate stage before the final solution is obtained. This requires us to evaluate two right hand sides, one for each stage, so is approximately twice as expensive as the explicit Euler method. However, it can be shown that it achieves $\mathcal{O}(\Delta t^2)$ in time. Hence, when the time step is small it is significantly more accurate.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\linewidth]{Pictures/ch15_heuns_method}
	\caption{Heun's method}
	\label{fig:heuns_method}
\end{figure}

\subsection{Midpoint Method}
The midpoint method is very similar to Heun's method. However, we start by computing the solution at an intermediate stage halfway between the current and next time steps, using an explicit Euler approach
\begin{equation}
	\tilde{u} = u^t + \frac{\Delta t}{2} R(u).
\end{equation}
Since we have divided $\Delta t$ by two, we are now halfway between the current and next time steps, hence why it is called the midpoint method. Now, we compute the solution at the next time step by evaluating the residual at this midpoint location
\begin{equation}
	u^{t+1} = u^t + \Delta t R(\tilde{u}).
\end{equation}
Similar to Heun's method, this is also a two stage scheme and has a temporal error of $\mathcal{O}(\Delta t^2)$.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\linewidth]{Pictures/ch15_midpoint_method}
	\caption{Midpoint method}
	\label{fig:midpoint_method}
\end{figure}

\subsection{Runge-Kutta Methods}
If we look at Heun's method and the midpoint method we note that their structure is very similar. We start by computing some intermediate solution, then evaluating the residual of that intermediate solution, and then linearly combining it with the residual of the initial solution. This can be generalized from two stages up to as many stages as we may like, with multiple intermediate solutions being obtained to improve the order of accuracy. Starting from our current solution $u^t$, we will compute $s$ intermediate stage solutions
\begin{equation}
	u_1, u_2, \hdots, u_s,
\end{equation}
where the subscript of $u_i$ denotes the stage number and $s$ is the total number of stages. From these we can also get the residuals of these intermediate solutions from $R(u_i)$. We compute the intermediate stage solutions from
\begin{equation}
	u_i = u^t + \Delta t \sum_{j=1}^s a_{i,j} R(u_j),
\end{equation}
and the final solution is found from
\begin{equation}
	u^{t+1} = u^t + \Delta t \sum_{i=1}^s b_i R(u_i),
\end{equation}
where $a_{i,j}$ and $b_i$ and a set of constants. These can be compacted into a {\it Butcher Tableau}
\begin{equation}
	\begin{tabular}{c|ccccc}
 
	c   & A \\
 
	\hline
 
	         & b\\
 
	\end{tabular}
    = 	
	\begin{tabular}{c|ccccc}
 
	$c_1$    & $a_{1,1}$ & $\cdots$ & $a_{1,s}$ \\
 
	$\vdots$ & $\vdots$ &  $\ddots$ & $\vdots$\\
 
	$c_s$    & $a_{s,1}$  & $\cdots$ & $a_{s,s}$\\
 
	\hline
 
	         & $b_1$  & $\cdots$ & $b_s$\\
 
	\end{tabular}
\end{equation}
This will yield an explicit scheme if the $A$ matrix is strictly lower-triangular. In that case, we find the solution at any stage is simple a function of the solution of earlier stages. In that case, the method consists of a number of intermediate stage solutions computed using an explicit Euler approach. The following are some example tableaus for commonly-used explicit schemes, including those already introduced.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\linewidth]{Pictures/ch15_rk_method}
	\caption{Fourth-order four-stage RK method}
	\label{fig:midpoint_method}
\end{figure}

\subsubsection{Explicit Euler}
The explicit Euler method has one stage and is $\mathcal{O}(\Delta t)$ in time.
\begin{equation}
	\begin{tabular}{c|ccccc}
	0   & 0 \\
	\hline
	         & 1\\
	\end{tabular}
\end{equation}

\subsubsection{Heun's Method}
Heun's method has two stages and is $\mathcal{O}(\Delta t^2)$ in time.
\begin{equation}
	\begin{tabular}{c|ccccc}
	0 & \\
	1 & 1 \\
	\hline
	 & $\frac{1}{2}$ & $\frac{1}{2}$\\
	\end{tabular}
\end{equation}

\subsubsection{Explicit Midpoint}
The explicit midpoint method has two stages and is $\mathcal{O}(\Delta t^2)$ in time.
\begin{equation}
	\begin{tabular}{c|ccccc}
	0   & \\
	$\frac{1}{2}$   & $\frac{1}{2}$ \\
	\hline
	         & 0 & 1\\
	\end{tabular}
\end{equation}


\subsubsection{Fourth-Order Runge-Kutta}
The explicit four-stage fourth-order Runge-Kutta method is $\mathcal{O}(\Delta t^4)$ in time.

\begin{equation}
	\begin{tabular}{c|ccccc}
	0   & \\
	$\frac{1}{2}$   & $\frac{1}{2}$ \\
	$\frac{1}{2}$   & 0 & $\frac{1}{2}$ \\
	$1$   & 0 & 0 & 1 \\
	\hline
	         & $\frac{1}{6}$ & $\frac{1}{3}$ & $\frac{1}{3}$ & $\frac{1}{6}$\\
	\end{tabular}
\end{equation}

\section{Implicit}
Going back to the von Neumann analysis section we note that implicit schemes, those where we evaluated the spatial derivative using the solution at the as-yet-unknown next time step, were unconditionally stable for both linear advection and diffusion. For example, the following scheme was introduced for the linear advection equation
\begin{equation}
	\frac{u_i^{t+1} - u_{i}^t}{\Delta t} +  \alpha \frac{u_i^{t+1} - u_{i-1}^{t+1}}{\Delta x} = 0,
\end{equation}
and the following for the linear diffusion equation
\begin{equation}
  \frac{u_i^{t+1} - u_{i}^t}{\Delta t} - \beta \frac{u_{i-1}^{t+1} - 2u_i^{t+1} + u_{i+1}^{t+1}}{\Delta x^2} = 0.
\end{equation}
These are very similar to our initial schemes, except the spatial operators are evaluated at $t+1$ rather than $t$.

\subsection{Implicit Linear Advection}
We will start our exploration of implicit schemes with the linear advection equation, and rearrange it so all of the terms involving the unknown solution at $t+1$ are of the left hand side and all of the terms involving the current solution at time $t$ are on the right hand side
\begin{equation}
	u_i^{t+1} + \sigma \left(u_i^{t+1} - u_{i-1}^{t+1} \right) = u_i^{t},
\end{equation}
where as usual $\sigma = \alpha \Delta t/ \Delta x$. Then, lumping the terms for each grid point on the left hand side yields
\begin{equation}
	\label{eqn:sh4k18g9}
	(1+\sigma) u_i^{t+1} - \sigma u_{i-1}^{t+1} = u_i^{t}.
\end{equation}
Unlike the explicit schemes, where we could readily rearrange the expression to get $u_i^{t+1}$ alone on the left hand side, we now have one equation for each grid point that has two unknowns, $u_i^{t+1}$ and $u_{i-1}^{t+1}$, on the left hand side. This means that in order to get $u_i^{t+1}$ we need to already know $u_{i-1}^{t+1}$, and in order to get $u_{i-1}^{t+1}$ we need to already know $u_{i-2}^{t+1}$, and so on. Hence, it appears we are stuck in a situation where we need to already know the solution at every grid point to get the solution at every other grid point, which we don't yet have.

To get around this we can recognize that we actually have a large system of linear equations. For each of the $N$ grid points there is an expresion of the form of Equation \ref{eqn:sh4k18g9}, and these involve some linear combination of the $N$ unknown solution values at $t+1$. Hence, we have $N$ equations and $N$ unknowns, yielding a linear system to be solved. This can be written as a linear system of the form
\begin{equation}
	\label{eqn:4h28f95h}
	A \vec{u}^{t+1} = \vec{u}^{t},
\end{equation} 
where, for the case of periodic boundary conditions
\begin{equation}
	A = 
	\begin{bmatrix}
	    1+\sigma & 0 & 0 & \dots  & \sigma \\
	    \sigma & 1+\sigma & 0 & \dots  & 0 \\
			0 & \sigma & 1+\sigma & \dots  & 0 \\
	    \vdots & \vdots & \vdots & \ddots & \vdots \\
	    0 & 0 & \dots & \sigma  & 1+\sigma
	\end{bmatrix},
\end{equation}
is referred to as the Jacobian matrix, and
\begin{equation}
	\vec{u}^{t} = \begin{bmatrix}
	    u_1^t \\
	    u_2^t \\
		u_3^t \\
	    \vdots  \\
	    u_N^t
	\end{bmatrix},
\end{equation}
is a vector of the solution at all grid points at the current time step, and
\begin{equation}
	\vec{u}^{t+1} = \begin{bmatrix}
	    u_1^{t+1} \\
	    u_2^{t+1} \\
		u_3^{t+1} \\
	    \vdots  \\
	    u_N^{t+1}
	\end{bmatrix},
\end{equation}
is a vector of the unknown solution values at the next time step, which is what we are trying to find. Note that the structure of the Jacobian matrix arises directly from writing out the equation for each grid point, and then assembling that as a linear system of equations. It is clear from Equation \ref{eqn:4h28f95h} that we can now solve for all of the solution values in $\vec{u}^{t+1}$ by simply solving a linear system of equations. Hence, we obtain unconditional stability, as discussed in the von Neumann analysis section, at the added cost of having to solve a linear system of equations at each time step. This cost is usually substantial and, hence, implicit schemes are typically used when sufficiently large time steps that are larger than the stability limits of explicit schemes are desired.

\subsection{Implicit Linear Diffusion}
As a second demonstration of an implicit solver we consider the implicit linear diffusion equation, derived previously as
\begin{equation}
  \frac{u_i^{t+1} - u_{i}^t}{\Delta t} - \beta \frac{u_{i-1}^{t+1} - 2u_i^{t+1} + u_{i+1}^{t+1}}{\Delta x^2} = 0.
\end{equation}
Rearranging this so that all of the known solution values are on the right hand side and all values at the as yet unknown next time step are on the left hand side yields
\begin{equation}
  u_i^{t+1} - r (u_{i-1}^{t+1} - 2u_i^{t+1} + u_{i+1}^{t+1}) = u_{i}^t,
\end{equation}
where again $r = \beta \Delta t / \Delta x^2$. Rearranging slight yields
\begin{equation}
  (1+2r)u_i^{t+1} - r (u_{i-1}^{t+1} + u_{i+1}^{t+1}) = u_{i}^t.
\end{equation}
Following the same steps as the linear advection equation, this can be written as a linear system of equations of the form
\begin{equation}
	A \vec{u}^{t+1} = \vec{u}^{t},
\end{equation}
where the Jacobian matrix for periodic boundary conditions can be written as
\begin{equation}
	A = 
	\begin{bmatrix}
	    1+2r & -r & 0 & \dots  & -r \\
	    -r & 1+2r & -r & \dots  & 0 \\
			0 & -r & 1+2r & \dots  & 0 \\
	    \vdots & \vdots & \vdots & \ddots & \vdots \\
	    -r & 0 & \dots & -r & 1+2r
	\end{bmatrix},
\end{equation}
taking note of the locations of the $-r$ terms in the first and last rows that arise from the periodic boundary conditions. Hence, similar to the implicit linear advection scheme, implicit linear diffusion requires the solution of a linear system of equations at each time step. Therefore, it is typically used when the desired time step exceeds the stability limit of available explicit methods. Due to the $\Delta x^2$ scaling of the time step size for diffusion equations, as discussed in the von Neumann analysis section, diffusion operators are often solved using implicit approaches.

\subsection{Implicit Burgers Equation}
In both of the previous sections we looked at implicit solvers for {\it linear} systems of equations, specifically the linear advection and linear diffusion equations. In this section we will consider an implicit solver for the {\it non-linear} Burgers equation with a numerical scheme of the form
\begin{equation}
	\frac{u_i^{t+1} - u_{i}^t}{\Delta t} + \frac{1}{2\Delta x}\left( \left(u_i^{t+1} \right)^2 - \left( u_{i-1}^{t+1} \right)^2 \right) =  0.
\end{equation}
Putting all of the known solution values on the right hand side and unknown solution values at the next time step on the left hand side yields
\begin{equation}
	u_i^{t+1} + \frac{\Delta t}{2\Delta x}\left( \left(u_i^{t+1} \right)^2 - \left( u_{i-1}^{t+1} \right)^2 \right) =  u_{i}^t,
\end{equation}
which rearranges to
\begin{equation}
	\left(1 + \frac{\Delta t}{2\Delta x}u_i^{t+1} \right)u_i^{t+1} - \left(\frac{\Delta t}{2\Delta x} u_{i-1}^{t+1}\right) u_{i-1}^{t+1}  =  u_{i}^t,
\end{equation}
and has the following Jacobian matrix
\begin{equation}
	A = 
	\begin{bmatrix}
	    1 + \frac{\Delta t}{2\Delta x}u_i^{t+1} & 0 & 0 & \dots  & - \left(\frac{\Delta t}{2\Delta x} u_{i-1}^{t+1}\right) \\
	    - \left(\frac{\Delta t}{2\Delta x} u_{i-1}^{t+1}\right) & 1 + \frac{\Delta t}{2\Delta x}u_i^{t+1} & 0 & \dots  & 0 \\
			0 & - \left(\frac{\Delta t}{2\Delta x} u_{i-1}^{t+1}\right) & 1 + \frac{\Delta t}{2\Delta x}u_i^{t+1} & \dots  & 0 \\
	    \vdots & \vdots & \vdots & \ddots & \vdots \\
	    0 & 0 & \dots & - \left(\frac{\Delta t}{2\Delta x} u_{i-1}^{t+1}\right)  & 1 + \frac{\Delta t}{2\Delta x}u_i^{t+1}
	\end{bmatrix},
\end{equation}
which is now a function of the unknown solution at the next time step. However, we do not yet know the solution at the next time step, so we cannot actually construct the Jacobian matrix and, therefore, we cannot solve directly for the solution at the next time step.

To get around this problem we can use {\it Newton-Rhapson} iterations. First assume that the Jacobian matrix can be {\it approximated} using the current known solution as
\begin{equation}
	A_0 = 
	\begin{bmatrix}
	    1 + \frac{\Delta t}{2\Delta x}u_i^{t} & 0 & 0 & \dots  & - \left(\frac{\Delta t}{2\Delta x} u_{i-1}^{t}\right) \\
	    - \left(\frac{\Delta t}{2\Delta x} u_{i-1}^{t}\right) & 1 + \frac{\Delta t}{2\Delta x}u_i^{t} & 0 & \dots  & 0 \\
			0 & - \left(\frac{\Delta t}{2\Delta x} u_{i-1}^{t}\right) & 1 + \frac{\Delta t}{2\Delta x}u_i^{t} & \dots  & 0 \\
	    \vdots & \vdots & \vdots & \ddots & \vdots \\
	    0 & 0 & \dots & - \left(\frac{\Delta t}{2\Delta x} u_{i-1}^{t}\right)  & 1 + \frac{\Delta t}{2\Delta x}u_i^{t}
	\end{bmatrix},
\end{equation}
where the subscript on $A_0$ denotes that this Jacobian is approximated using the current known solution at time $t$. This is referred to as a {\it linearization} of the system of equations, and we say that the Jacobian matrix has been linearized about the current solution $\vec{u}^t$. From this linearization the solution at $t+1$ can be estimated by solving the linear system
\begin{equation}
	A_0 \vec{u}^{t+1}_1 = \vec{u}^t,
\end{equation}
where the subscript in $\vec{u}^{t+1}_1$ denotes that this is the first iterated approximation of the solution at the next time step. Then, the Jacobian matrix can be approximated by linearizing it about this first estimated solution instead of $\vec{u}^t$, yielding the following linear system of equations
\begin{equation}
	A_1 \vec{u}^{t+1}_2 = \vec{u}^t,
\end{equation}
and this procedure can be repeated in the following form
\begin{equation}
	A_k \vec{u}^{t+1}_{k+1} = \vec{u}^t,
\end{equation}
where $k$ denotes the current iteration. This procedure is repeated until
\begin{equation}
	\vec{u}^{t+1}_{k+1} \approx \vec{u}^{t+1}_{k},
\end{equation}
at which point the solution at the next time step is taken to be
\begin{equation}
	\vec{u}^{t+1} = \vec{u}^{t+1}_{k+1},
\end{equation}
and the entire procedure is then repeated for each subsequent time step. Hence, for non-linear equations we need to solve multiple linear systems for each time step, following Newton-Rhapson, which can significantly increase their computational cost relative to linear systems. Furthermore, convergence of Newton-Rhapson is not guaranteed. Hence, non-linear systems often require smaller time steps relative to linear systems to maintain stability of the iterative method.

\chapter{Iterative Methods}
In this implicit time stepping section we reduced each time step to the solution of a linear system of $N$ equations of the form 
\begin{equation}
	A \vec{x} = \vec{b},
\end{equation}
when solving either linear advection or linear diffusion, where $N$ is the number of grid points in the domain. The extra cost of solving this linear system was introduced in order to obtain unconditionally stable schemes, allowing very large time steps to be taken. However, this will only be faster than an explicit approach if the linear system of equations can be solved efficiently. Hence, this section is dedicated to exploring different methods of solving linear systems of equations.

\section{Gaussian Elimination}
Perhaps the most straightforward method for solving a linear system of equations is Guassian elimination. This requires us to invert $A$ and obtain
\begin{equation}
	 \vec{x} = A^{-1} \vec{b}.
\end{equation}
Gaussian elimination is the workhorse of most undergraduate linear algebra courses, and is a straightforward extension of the substitution approach for solving systems of equations taught in highschool. This requires inverting $A$ or an LU decomposition of $A$ and $\vec{b}$ to be formed, and then backsubstitution to solve for the unknown components of $\vec{x}$.

While Gaussian elimination is well known, its computational cost scales like $\mathcal{O}(N^3)$ where $N$ is the number of unknowns. Hence, as the system of equations, and in our case number of grid points, grows, the computational cost of Gaussian elimination grows cubicly. Any undergraduate student should be familiar with trying to solve even $3 \times 3$ linear systems in an exam, let alone $4 \times 4$, or $5 \times 5$. With each additional equation, doing this by hand takes significantly more time. Hence, in the context of CFD, where thousands or millions of equations is common practice, Gaussian elimination is rarely used.

\section{Jacobi Iteration}
Since the formation of $A^{-1}$ is prohibitively expensive for large systems of equations, we will try to find an approximate solution $\vec{x}$, without actually forming $A^{-1}$. This can be done iteratively, and the first approach we will introduce is Jacobi iteration. In Jacobi iteration we start by splitting $A$ into $L$, $D$, and $U$, and are its strictly lower-triangular, diagonal, and strictly upper-triangular components. This allows us to write our linear system as
\begin{equation}
	(L+D+U) \vec{x} = \vec{b},
\end{equation}
where
\begin{equation}
	A = 
	\begin{bmatrix}
	    a_{11} & a_{12} & a_{13} & \dots  & a_{1n} \\
	    a_{21} & a_{22} & a_{23} & \dots  & a_{2n} \\
			a_{31} & a_{32} & a_{33} & \dots  & a_{3n} \\
	    \vdots & \vdots & \vdots & \ddots & \vdots \\
	    a_{n1} & a_{n2} & a_{n3} & \dots  & a_{nn}
	\end{bmatrix},
\end{equation}
\begin{equation}
	L = 
	\begin{bmatrix}
	    0 & 0 & 0 & \dots  & 0 \\
	    a_{21} & 0 & 0 & \dots  & 0 \\
			a_{31} & a_{32} & 0 & \dots  & 0 \\
	    \vdots & \vdots & \vdots & \ddots & \vdots \\
	    a_{n1} & a_{n2} & a_{n3} & \dots  & 0
	\end{bmatrix},
\end{equation}
\begin{equation}
	D = 
	\begin{bmatrix}
	    a_{11} & 0 & 0 & \dots  & 0 \\
	    0 & a_{22} & 0 & \dots  & 0 \\
			0 & 0 & a_{33} & \dots  & 0 \\
	    \vdots & \vdots & \vdots & \ddots & \vdots \\
	    0 & 0 & 0 & \dots  & a_{nn}
	\end{bmatrix},
\end{equation}
\begin{equation}
	U = 
	\begin{bmatrix}
	    0 & a_{12} & a_{13} & \dots  & a_{1n} \\
	    0 & 0 & a_{23} & \dots  & a_{2n} \\
			0 & 0 & 0 & \dots  & a_{3n} \\
	    \vdots & \vdots & \vdots & \ddots & \vdots \\
	    0 & 0 & 0 & \dots  & 0
	\end{bmatrix}.
\end{equation}
Now if $\vec{x}$ is a solution to the linear system then the following is valid
\begin{equation}
	L\vec{x} + D\vec{x} + U\vec{x} = \vec{b}.
\end{equation}
We can now rearrange this expression
\begin{equation}
	D\vec{x} = \vec{b} - (L+U)\vec{x},
\end{equation}
and by inverting $D$ we get
\begin{equation}
	\vec{x} = D^{-1}\left[ \vec{b} - (L+U)\vec{x} \right].
	\label{eqn:f928bs7g}
\end{equation}
At this point it is important to now that while inverting $A$ is very expensive, inverting just its diagonal values in $D$ is trivial.

We note that if $\vec{x}$ is a solution to the linear system of equations, then Equation \ref{eqn:f928bs7g} will be satisfied exactly, but if we already had the exact solution there would be no point in this exercise. However, if we look again at Equation \ref{eqn:f928bs7g} it has an interesting form. We note that if we insert some guess for $\vec{x}$ into the right hand side, we get out a modified $\vec{x}$ on the left hand side. This allows us to define Jacobi iteration as
\begin{eqBox}
\begin{equation}
	\vec{x}_{n+1} = D^{-1}\left[ \vec{b} - (L+U)\vec{x}_n \right],
	\label{eqn:3jc810fg}
\end{equation}
\end{eqBox}
where $\vec{x}_n$ is an approximate solution, and $\vec{x}_{n+1}$ is an updated approximation. Then we can simply pass $\vec{x}_{n+1}$ back through this formulation to obtain $\vec{x}_{n+2}$, and so on, iterating to a final solution when the value of $\vec{x}$ converges. This can also be written in an element-by-element manner as
\begin{eqBox}
\begin{equation}
	x_i^{n+1} = \frac{1}{a_{i,i}}\left(b_i - \sum_{i \neq i} a_{i,j}x_j^k \right), \: \: i = 1,2,\hdots,n.
\end{equation}
\end{eqBox}
Each step of Jacobi iteration requires $\mathcal{O}(N^2)$ operations, hence, we can expect Jacobi to outperform Gaussian elimination, as long as we need fewer iterations than there are rows in the system of equations. Furthermore, in CFD it is usually sufficient to converge $\vec{x}$ to some finite level of precision.

To demonstrate the utility of Jacobi iteration we will use a simple example of a $3 \times 3$ system of equations. Starting with 
\begin{equation}
	A = 
	\begin{bmatrix}
	    10 & 2 & 4 \\
	    6 & 8 & 4 \\
			2 & 3 & 9
	\end{bmatrix},
\end{equation}
and
\begin{equation}
	\vec{b} = \begin{bmatrix}
	    1 \\
	    2 \\
			3
	\end{bmatrix},
\end{equation}
we obtain the following for $L$, $D$, and $U$
\begin{equation}
	L = 
	\begin{bmatrix}
	    0 & 0 & 0 \\
	    6 & 0 & 0 \\
			2 & 3 & 0
	\end{bmatrix},
\end{equation}
\begin{equation}
	D = 
	\begin{bmatrix}
	    10 & 0 & 0 \\
	    0 & 8 & 0 \\
			0 & 0 & 9
	\end{bmatrix},
\end{equation}
\begin{equation}
	U = 
	\begin{bmatrix}
	    0 & 2 & 4 \\
	    0 & 0 & 4 \\
			0 & 0 & 0
	\end{bmatrix}.
\end{equation}
Furthermore, we can easily invert $D$ to get
\begin{equation}
	D^{-1} = 
	\begin{bmatrix}
	    \frac{1}{10} & 0 & 0 \\
	    0 & \frac{1}{8} & 0 \\
			0 & 0 & \frac{1}{9}
	\end{bmatrix}.
\end{equation}
Now we can start with some initial guess, lets say
\begin{equation}
	\vec{x_0} = \begin{bmatrix}
	    0 \\
	    0 \\
			0
	\end{bmatrix}.
\end{equation}
Convergence can usually be accelerated by using a good initial guess, for example, that the solution at the next time step in our CFD simulation is the same as the current solution.

Now with our initial Guess and all terms from the right hand side of Equation \ref{eqn:3jc810fg} defined, we can compute
\begin{equation}
	\vec{x}_{1} = D^{-1}\left[ \vec{b} - (L+U)\vec{x}_0 \right] = \begin{bmatrix}
	    0.1000 \\
	    0.2500 \\
			0.3333
	\end{bmatrix}.
\end{equation}
Repeating this iteration a few times yields
\begin{equation}
	\vec{x}_{2} = D^{-1}\left[ \vec{b} - (L+U)\vec{x}_1 \right] = \begin{bmatrix}
	    -0.0833 \\
	    0.0083 \\
			0.2277
	\end{bmatrix},
\end{equation}
\begin{equation}
	\vec{x}_{3} = D^{-1}\left[ \vec{b} - (L+U)\vec{x}_2 \right] = \begin{bmatrix}
	    0.0072 \\
	    0.1986 \\
			0.3490
	\end{bmatrix},
\end{equation}
and after 15 iterations we obtain
\begin{equation}
	\vec{x}_{15} = D^{-1}\left[ \vec{b} - (L+U)\vec{x}_14 \right] = \begin{bmatrix}
	    -0.0465 \\
	    0.1356 \\
			0.2984
	\end{bmatrix},
\end{equation}
which is very close to the exact solution
\begin{equation}
	\vec{x} = \begin{bmatrix}
	    -0.0465 \\
	    0.1357 \\
			0.2984
	\end{bmatrix}.
\end{equation}
Hence, after only 15 iterations Jacobi was able to converge to approximately four digits, without having to ever directly invert the $A$ matrix.

\section{Gauss Seidel Iteration}
With the idea of Jacobi iteration outlined above, we may wonder whether similar more efficient approaches exist. The first of these, with a very similar derivation to Jacobi, is the Gauss-Seidel method. Starting from our split $A$ matrix in the Jacobi section
\begin{equation}
	(L+D+U) \vec{x} = \vec{b},
\end{equation}
we rearrange it as
\begin{equation}
	(L + D)\vec{x} = \vec{b} - U\vec{x}.
\end{equation}
If we consider the $(L+D)$ matrix we note that it is lower-triangular. While not as trivial to invert as $D$ in Jacobi iteration, inverting $(L+D)$ can be done quickly via back-substitution. Hence, we can write
\begin{equation}
	\vec{x} = (L + D)^{-1}\left[ \vec{b} - U\vec{x} \right].
\end{equation}
Similar to Jacobi, we notice that if we insert an estimate for $\vec{x}$ on the right hand side, we obtain an updated estimate on the left hand side. Hence, we define Gauss-Seidel iteration as
\begin{eqBox}
\begin{equation}
	\vec{x}_{n+1} = (L + D)^{-1}\left[ \vec{b} - U\vec{x}_n \right].
\end{equation}
\end{eqBox}
Also, similar to Jacobi this can be performed in a element-wise manner for each component of the solution via
\begin{eqBox}
\begin{equation}
	x_i^{n+1} = \frac{1}{a_{i,i}}\left(b_i - \sum_{j=1}^{i-1} a_{i,j}x_j^{n+1} - \sum_{j=i+1} a_{i,j}x_j^n \right), \: \: i = 1,2,\hdots,n.
\end{equation}
\end{eqBox}
It is clear from this that Gauss-Seidel amounts to simply a Jacobi iteration but using the most up to date entries $a_{i,j}x_j^n$ or $a_{i,j}x_j^{n+1}$ as they are available. Usually the additional cost of computing $(L + D)^{-1}$, relative to simply computing $D^{-1}$ with Jacobi, significantly reduces the number of iterations and total computational cost.

Going back to our example from Jacobi iteration we have
Now with our initial Guess and all terms from the right hand side of Equation \ref{eqn:3jc810fg} defined, we can compute
\begin{equation}
	\vec{x}_{1} = (L + D)^{-1}\left[ \vec{b} - U\vec{x}_0 \right] = \begin{bmatrix}
	    0.1000 \\
	    0.1750 \\
			0.2527
	\end{bmatrix}.
\end{equation}
Repeating this iteration a few times yields
\begin{equation}
	\vec{x}_{2} = (L + D)^{-1}\left[ \vec{b} - U\vec{x}_1 \right] = \begin{bmatrix}
	    -0.0361 \\
	    0.1506 \\
			0.2911
	\end{bmatrix},
\end{equation}
\begin{equation}
	\vec{x}_{3} = (L + D)^{-1}\left[ \vec{b} - U\vec{x}_2 \right] = \begin{bmatrix}
	    -0.0465 \\
	    -0.0467 \\
			0.2982
	\end{bmatrix},
\end{equation}
and after only 5 iterations we obtain
\begin{equation}
		\vec{x}_{5} = (L + D)^{-1}\left[ \vec{b} - U\vec{x}_4 \right] = \begin{bmatrix}
	    -0.0465 \\
	    0.1358 \\
			0.2984
	\end{bmatrix},
\end{equation}
which is very close to the exact solution
\begin{equation}
	\vec{x} = \begin{bmatrix}
	    -0.0465 \\
	    0.1357 \\
			0.2984
	\end{bmatrix}.
\end{equation}
Hence, after only 5 iterations, or three times as fast as Jacobi in this case, Gauss Seidel was able to converge to approximately four digits.

\section{Successive Over-Relaxation}
When using the Gauss-Seidel approach, we often observe that the iterated solutions converge gradually to the exact solution for $\vec{x}$. That is, each iteration takes a step towards the exact solution in a somewhat uniform manner. We can exploit this by simply extrapolating each iteration to move a bit further towards the exact solution, which is known as Successive Over-Relaxation (SOR). This is usually done to accelerate convergence, but as will be discussed later, it can be used to also stabilize convergence.

With SOR we simply apply a Gauss Seidel iteration to our current approximation of the solution, $\vec{x}_n$. We store the results of this Gauss-Seidel iteration temporarily as $\tilde{\vec{x}}_{n+1}$. Then, using the SOR approach, we linearly project our updated solution using our current and previous approximations, that is
\begin{eqBox}
\begin{equation}
	\vec{x}_{n+1} = \omega \tilde{\vec{x}}_{n+1} + (1-\omega) \vec{x}_n.
\end{equation}
\end{eqBox}
where $\omega$ is referred to as the the relaxation factor. We note that when $\omega=1$ we recover the original Gauss Seidel approach. When $\omega>1$ we say it is over-relaxed, usually accelerating convergence but also potentially causing the iterations to diverge. When $\omega<1$ we say it is under-relaxed, which usually improves stability, but reduces the rate of convergence. In general, a suitable value for the relaxation factor is within the range
\begin{equation}
	0 \leq \omega \leq 2,
\end{equation}
and values of $\omega > 2$ will diverge. Typically, $\omega$ is chosen to be as large as possible, without causing the iterations to diverge.

\section{Assessing Convergence}
In typical CFD applications, the user provides a desired convergence tolerance for solving the linear system of equations. To assess this, we introduce the concept of a residual. In the case of an exact solution to a linear system we would expect
\begin{equation}
	A \vec{x} - \vec{b} = 0.
\end{equation}
However, since our solution at any given iteration is only an approximate solution then
\begin{equation}
	A \vec{x}_n - \vec{b} = \vec{r},
\end{equation}
where $\vec{r}$ is referred to as the residual and measures how well our current approximate solution satisfies the system of equations being solved. While $\vec{r}$ gives detailed information about how well each equation in the entire system is being solved, it is typically more useful to provide the user with a norm, such as $\| \vec{r} \|_2$ or $\| \vec{r} \|_\infty$, giving a single measure of convergence. Then, iterations are continued until this residual norm converges to within the desired tolerance. It is also important to note that if the convergence tolerance is too high it can results in a loss of accuracy or stability in the CFD simulation. Hence, particular care must be chosen in how to select the desired convergence tolerance.

\section{Multigrid}

\chapter{Applications}
\section{An Euler Solver}

\section{A Navier-Stokes Solver}